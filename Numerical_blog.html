<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="Kory.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Kory Illenye</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Teaching
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Class_2.html">STT 2810</a>
    </li>
    <li>
      <a href="Class_1.html">MAT 1110</a>
    </li>
    <li>
      <a href="Resources.html">Resources</a>
    </li>
    <li>
      <a href="Numerical_blog.html">Numerical Blog</a>
    </li>
  </ul>
</li>
<li>
  <a href="Contact.html">Contact</a>
</li>
<li>
  <a href="http://mathsci.appstate.edu">Department of Mathematical Sciences</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<div id="steepest-descent-and-steepest-descent-with-least-squares." class="section level1">
<h1>Steepest Descent and Steepest Descent with Least Squares.</h1>
<p>Initially we started by looking at two iterative methods for finding solutions to positive definite, symmetric matrices. Steepest Descent is often called <strong>gradient descent</strong>, which is a approach to finding minima of a function <span class="math inline">\(f\)</span>. Using an estimate <span class="math inline">\(\vec{x}_i\)</span>, the negative gradient <span class="math inline">\(-\nabla f(x_i)\)</span> gives the direction of in which <span class="math inline">\(f\)</span> is decreasing at the greatest rate. The goal is to take steps in this direction until we react the minimum.</p>
<p>Notation:</p>
<p><span class="math inline">\(e\)</span> denotes an error term</p>
<p><span class="math inline">\(i\)</span> denotes the i-ith term</p>
<p><span class="math inline">\(r\)</span> denotes a residual</p>
<p><span class="math display">\[\vec{e}_i = \vec{x}_i - \vec{x}\]</span> <span class="math display">\[\vec{r}_i = \vec{b} - A\vec{x}_i = -A\vec{e}_i\]</span></p>
<p>To determine the size, often referred to as <span class="math inline">\(\alpha\)</span>, a ratio of inner products from residuals is used. <span class="math inline">\(\vec{x}_{i+1} = \vec{x}_i - \alpha_i\nabla f(\vec{x}_i)\)</span> this minimizes <span class="math inline">\(f(\vec{x}_{i+1})\)</span>. Now, that it is understood what is needed lets define a few things. To be symmetric means <span class="math inline">\(A = A^T\)</span> where <span class="math inline">\(A^T\)</span> is the transpose of A. Positive definite means all of the matrix’s eigen values are greater than 0.</p>
<div id="creating-a-positive-definite-symetric-matrix" class="section level2">
<h2>Creating a positive definite, symetric matrix</h2>
<p>Defining initial variables.</p>
<pre class="r"><code># Setting the seed to work with 
set.seed(31)
# Number of columns and rows in the matrix in the matrix
n &lt;- 5 
# Maximum number of iterations for functions
maxIts &lt;- 1000000
# Setting a tolerance for use with the function
tol = 10^-5
# Creates a random nxn matrix with n^2 randomly generated numbers
A&lt;-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
A &lt;- t(A) %*% A 
# Creates a random nx1 solution vector to solve for
xa &lt;- matrix(runif(n), ncol = 1)
# Creates an nx1 b vector from the random vector x
a &lt;- A %*% xa
# Initial approximation vector (zero vecor)
x0a &lt;- matrix(rep(0.0,n), ncol = 1)
# Determines if all the eigen values are positive
is.positive.definite(A)</code></pre>
<pre><code>[1] TRUE</code></pre>
<p>The return of TRUE from the code block above allows us to know that the matrix is positive definite. Lets see what these objects look like and then design the function.</p>
<div id="the-matrix-a" class="section level3">
<h3>The Matrix A</h3>
<pre class="r"><code># Prints the matrix A
A</code></pre>
<pre><code>          [,1]      [,2]      [,3]      [,4]      [,5]
[1,] 2.4299775 1.7269373 2.0199944 0.5590044 1.5099956
[2,] 1.7269373 1.8590925 1.1651029 0.4614977 1.1855963
[3,] 2.0199944 1.1651029 1.8962176 0.5224703 1.2304534
[4,] 0.5590044 0.4614977 0.5224703 0.3137412 0.4209155
[5,] 1.5099956 1.1855963 1.2304534 0.4209155 1.0294046</code></pre>
</div>
<div id="the-b-vector" class="section level3">
<h3>The b vector</h3>
<pre class="r"><code># Prints b vector for A
a</code></pre>
<pre><code>          [,1]
[1,] 2.2159566
[2,] 1.4658554
[3,] 1.9866331
[4,] 0.6013222
[5,] 1.3916532</code></pre>
<p>Without question the code is giving us the desired outcome of a <span class="math inline">\(5x5\)</span> matrix and a <span class="math inline">\(5x1\)</span> solution vector.</p>
</div>
</div>
</div>
<div id="the-funtion-for-gradient-descent-method" class="section level1">
<h1>The Funtion for Gradient Descent Method</h1>
<p>This function uses a ratio of residual vectors as a step size.</p>
<p><span class="math display">\[\alpha = \frac{&lt;\vec{r}^T,\vec{r}&gt;}{\vec{r}^TA\vec{r}}\]</span> Where <span class="math inline">\(\vec{r}\)</span> is the previous iterations residual vector.</p>
<p>We approximate <span class="math inline">\(\vec{x}\)</span> by the following equation:</p>
<p><span class="math display">\[\vec{x_{n+1}} = \vec{x_n} + \alpha\vec{r}\]</span> Here is the code for gradient descent in <code>R</code>.</p>
<pre class="r"><code># Defines the function
grad &lt;- function (A,b,x0a, maxIts, tol, x)
  {
    # Calculates n
    n &lt;- length(b)
    # Creates an initial residual
    r &lt;- b - (A %*% x0a)
    # Used to keep track of the number of iterations.
    k = 0
    # Loop used to attempt to find the approximation of the X vector
    while (k &lt; maxIts &amp;&amp; norm(r, &quot;1&quot;) &gt; tol)
    {
      # Increments the iteration
      k &lt;- k+1
      # Creates a step size using the residuals and matrix A
      alpha &lt;- (t(r) %*% r) / (t(r) %*% (A %*% r))
      # Creates the next approximation of the X vector
      x0a &lt;- x0a + drop(alpha) * r
      # Creates the new residual with the new approximation
      r &lt;- b - A %*% x0a
    }
  # Calculates the residual vector.
  res &lt;- x - x0a
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol &lt;- cbind(x, x0a, res, matrix(rep(k,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) &lt;- c(&quot;X&quot;, &quot;X0&quot;,&quot;resid&quot;, &quot;iter&quot;)
  # Prints the array
  sol
 
}</code></pre>
<p>This code when run generates the following:</p>
<pre class="r"><code># Runs the function and saves it to a variable &quot;c&quot;
solgradA &lt;- grad(A, a, x0a, maxIts, tol, xa)
# Prints the array 
solgradA</code></pre>
<pre><code>              X         X0         resid iter
[1,] 0.04876597 0.04895561 -1.896346e-04  325
[2,] 0.14752424 0.14746647  5.776931e-05  325
[3,] 0.81504717 0.81491134  1.358297e-04  325
[4,] 0.16098630 0.16109232 -1.060204e-04  325
[5,] 0.07040319 0.07030940  9.378641e-05  325</code></pre>
<p>You will notice that the code is not random and reproduces the same number every time. This is because a random seed was selected for reproduction of results. The <span class="math inline">\(X\)</span> column represents the exact solution, where the <span class="math inline">\(X0\)</span> column represents the approximation from the function. The resid column shows the actual difference between the two vectors. As you can see the approximation is very close to the actual solution. The norm of the residual is <span class="math inline">\(10^5\)</span> when rounded to 5 decimal places. For this document we know that if the <code>iter</code> column is not equal to 10^{6} then the maximum norm of the residual vector is <span class="math inline">\(10^5\)</span>. The final column just represents the number of iterations it actually took the function before converging to the approximate solution. You will notice that each row has an out put but this does <strong>not</strong> mean each row took that many iterations. Simply was concatenated this way. On the above matrix it took the function 325 iterations to complete. It worked for a <span class="math inline">\(5x5\)</span> matrix, now to apply it to a few larger matrices.</p>
<div id="x6-positive-definite-symmetric-matrix" class="section level2">
<h2>6x6 Positive Definite Symmetric Matrix</h2>
<pre class="r"><code>set.seed(21)
n &lt;- 6 
# Creates a Random matrix with n^2 randomly generated numbers
B&lt;-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
B &lt;- t(B) %*% B
# Creates a random solution vector to solve for
xb &lt;- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
b &lt;- B %*% xb
# Initial approximation vector (zero vecor)
x0b &lt;- matrix(rep(0.0,n), ncol = 1)
# Determines if all the eigen values are positive
is.positive.definite(B)</code></pre>
<pre><code>[1] TRUE</code></pre>
<div id="the-b-matrix-and-b-vector" class="section level3">
<h3>The B Matrix and b vector</h3>
<pre class="r"><code>B</code></pre>
<pre><code>         [,1]      [,2]      [,3]     [,4]     [,5]     [,6]
[1,] 2.969523 2.4693875 1.0945040 2.265522 2.268699 2.549656
[2,] 2.469388 3.0539191 0.9314847 2.069346 2.489122 2.389834
[3,] 1.094504 0.9314847 0.7736687 1.114188 1.197342 1.040629
[4,] 2.265522 2.0693460 1.1141883 2.475676 2.296733 2.158044
[5,] 2.268699 2.4891220 1.1973418 2.296733 2.593211 2.497518
[6,] 2.549656 2.3898338 1.0406293 2.158044 2.497518 3.180717</code></pre>
<pre class="r"><code>b</code></pre>
<pre><code>         [,1]
[1,] 7.708481
[2,] 7.815830
[3,] 3.454047
[4,] 7.049440
[5,] 7.781529
[6,] 8.252008</code></pre>
</div>
<div id="the-results" class="section level3">
<h3>The Results</h3>
<pre class="r"><code>solgradB&lt;-grad(B, b, x0b, maxIts, tol,xb)
solgradB</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.2163319 0.2164881 -0.0001561833 1781
[2,] 0.6504235 0.6502254  0.0001980371 1781
[3,] 0.3351660 0.3348874  0.0002786669 1781
[4,] 0.5076559 0.5075127  0.0001432318 1781
[5,] 0.6528394 0.6532767 -0.0004373487 1781
[6,] 0.9655767 0.9654451  0.0001315399 1781</code></pre>
<p>For the purpose of this document random seeds have been set for replication and discussion but many of random matrices were sampled repeatedly but not reported. Each of the following seeds were created by a random 10-sided dice and adding a digit to the previous seed.</p>
</div>
</div>
<div id="another-6x6-positive-definite-symmetric-matrix" class="section level2">
<h2>Another 6x6 Positive Definite Symmetric Matrix</h2>
<pre class="r"><code>set.seed(214)
n &lt;- 6 
# Creates a Random matrix with n^2 randomly generated numbers
C&lt;-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
C &lt;- t(C) %*% C
# Creates a random solution vector to solve for
xc &lt;- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
c &lt;- C %*% xc
# initial approximation vector (zero vecor)
x0c &lt;- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(C)</code></pre>
<pre><code>[1] TRUE</code></pre>
<div id="the-a-matrix-and-b-vector" class="section level3">
<h3>The A Matrix and b vector</h3>
<pre class="r"><code>C</code></pre>
<pre><code>         [,1]     [,2]      [,3]      [,4]      [,5]      [,6]
[1,] 2.591707 2.217206 1.1814162 1.5399616 1.6181376 1.1860408
[2,] 2.217206 2.650669 1.6894440 2.0397363 1.4540539 1.1738115
[3,] 1.181416 1.689444 1.3483715 1.3084386 0.9020424 0.5422738
[4,] 1.539962 2.039736 1.3084386 2.0160104 1.2108004 0.8519311
[5,] 1.618138 1.454054 0.9020424 1.2108004 1.4474137 0.4330670
[6,] 1.186041 1.173812 0.5422738 0.8519311 0.4330670 0.9206222</code></pre>
<pre class="r"><code>c</code></pre>
<pre><code>         [,1]
[1,] 5.051013
[2,] 5.395403
[3,] 3.358559
[4,] 4.480104
[5,] 3.678552
[6,] 2.320329</code></pre>
</div>
<div id="the-results-1" class="section level3">
<h3>The Results</h3>
<pre class="r"><code>solgradC&lt;-grad(C, c, x0c, maxIts, tol,xc)
solgradC</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.6807363 0.6806130  1.233517e-04 1419
[2,] 0.1331188 0.1331240 -5.250738e-06 1419
[3,] 0.4157805 0.4157955 -1.500242e-05 1419
[4,] 0.8683657 0.8683055  6.016602e-05 1419
[5,] 0.6214230 0.6215572 -1.341803e-04 1419
[6,] 0.1328657 0.1330050 -1.392501e-04 1419</code></pre>
</div>
</div>
<div id="another-6x6-positive-definite-symmetric-matrix-1" class="section level2">
<h2>Another 6x6 Positive Definite Symmetric Matrix</h2>
<pre class="r"><code>set.seed(2145)
n &lt;- 6 
# Creates a Random matrix with n^2 randomly generated numbers
D&lt;-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
D &lt;- t(D) %*% D
# Creates a random solution vector to solve for
xd &lt;- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
d &lt;- D %*% xd
# initial approximation vector (zero vecor)
x0d &lt;- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(D)</code></pre>
<pre><code>[1] TRUE</code></pre>
<div id="the-d-matrix-and-b-vector" class="section level3">
<h3>The D Matrix and b vector</h3>
<pre class="r"><code>D</code></pre>
<pre><code>         [,1]      [,2]      [,3]     [,4]     [,5]     [,6]
[1,] 1.629180 1.3253076 1.3704596 1.897338 1.492470 1.388390
[2,] 1.325308 2.1268329 0.9046964 2.031244 1.432492 1.145834
[3,] 1.370460 0.9046964 1.6388220 1.852486 1.570883 1.469332
[4,] 1.897338 2.0312444 1.8524862 2.844479 2.251959 1.748982
[5,] 1.492470 1.4324924 1.5708827 2.251959 2.753722 1.775626
[6,] 1.388390 1.1458340 1.4693319 1.748982 1.775626 1.692876</code></pre>
<pre class="r"><code>d</code></pre>
<pre><code>         [,1]
[1,] 6.472100
[2,] 5.990356
[3,] 6.497666
[4,] 8.940155
[5,] 8.389341
[6,] 6.779419</code></pre>
</div>
<div id="the-results-2" class="section level3">
<h3>The Results</h3>
<pre class="r"><code>solgradD&lt;- grad(D, d, x0d, maxIts, tol,xd)
solgradD</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.6258299 0.6258653 -3.544603e-05 1515
[2,] 0.2011419 0.2014055 -2.636294e-04 1515
[3,] 0.5883483 0.5887837 -4.354756e-04 1515
[4,] 0.8560448 0.8556266  4.181889e-04 1515
[5,] 0.9363852 0.9364996 -1.144587e-04 1515
[6,] 0.9780356 0.9777614  2.741660e-04 1515</code></pre>
</div>
</div>
<div id="another-6x6-positive-definite-symmetric-matrix-2" class="section level2">
<h2>Another 6x6 Positive Definite Symmetric Matrix</h2>
<pre class="r"><code>set.seed(21458)
n &lt;- 6 
# Creates a Random matrix with n^2 randomly generated numbers
E&lt;-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
E &lt;- t(E) %*% E
# Creates a random solution vector to solve for
xe &lt;- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
e &lt;- E %*% xe
# initial approximation vector (zero vecor)
x0e &lt;- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(E)</code></pre>
<pre><code>[1] TRUE</code></pre>
<div id="the-e-matrix-and-b-vector" class="section level3">
<h3>The E Matrix and b vector</h3>
<pre class="r"><code>E</code></pre>
<pre><code>          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]
[1,] 0.8516702 0.5918866 0.9644886 0.2638436 0.8583120 1.2921903
[2,] 0.5918866 1.5296755 1.6492970 0.1676149 1.6131301 1.1953723
[3,] 0.9644886 1.6492970 3.0691736 1.0978719 2.0166848 1.7108179
[4,] 0.2638436 0.1676149 1.0978719 0.9585854 0.3851128 0.4277445
[5,] 0.8583120 1.6131301 2.0166848 0.3851128 2.1906219 1.6998282
[6,] 1.2921903 1.1953723 1.7108179 0.4277445 1.6998282 2.3113791</code></pre>
<pre class="r"><code>e</code></pre>
<pre><code>         [,1]
[1,] 2.246422
[2,] 3.265114
[3,] 4.870010
[4,] 1.403840
[5,] 4.279366
[6,] 4.070408</code></pre>
</div>
<div id="the-results-3" class="section level3">
<h3>The Results</h3>
<pre class="r"><code>solgradE&lt;- grad(E, e, x0e, maxIts, tol,xe)
solgradE</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.4544894 0.4544451  4.431286e-05  322
[2,] 0.3672737 0.3672618  1.191118e-05  322
[3,] 0.4045790 0.4045915 -1.246805e-05  322
[4,] 0.3278605 0.3278489  1.160496e-05  322
[5,] 0.7741572 0.7741505  6.730072e-06  322
[6,] 0.3875421 0.3875714 -2.928487e-05  322</code></pre>
</div>
</div>
</div>
<div id="a-10x10-positive-definite-symmetric-matrix" class="section level1">
<h1>A 10x10 Positive Definite Symmetric Matrix</h1>
<pre class="r"><code>set.seed(21456)
n &lt;- 10 
# Creates a Random matrix with n^2 randomly generated numbers
F&lt;-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
F &lt;- t(F) %*% F
# Creates a random solution vector to solve for
xf &lt;- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
f &lt;- F %*% xf
# initial approximation vector (zero vecor)
x0f &lt;- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(F)</code></pre>
<pre><code>[1] TRUE</code></pre>
<div id="the-f-matrix-and-b-vector" class="section level3">
<h3>The F Matrix and b vector</h3>
<pre class="r"><code>F</code></pre>
<pre><code>          [,1]     [,2]      [,3]     [,4]     [,5]      [,6]     [,7]
 [1,] 4.576307 3.544017 1.8706107 2.699453 3.464121 2.1412326 3.630429
 [2,] 3.544017 3.995759 1.7402252 2.908020 3.122860 1.7878213 3.701816
 [3,] 1.870611 1.740225 1.4091612 1.632820 1.585560 0.8950613 1.883202
 [4,] 2.699453 2.908020 1.6328204 3.177959 2.352454 1.6335524 2.982156
 [5,] 3.464121 3.122860 1.5855600 2.352454 3.832469 2.1929775 3.852252
 [6,] 2.141233 1.787821 0.8950613 1.633552 2.192978 2.0398368 2.512283
 [7,] 3.630429 3.701816 1.8832023 2.982156 3.852252 2.5122828 5.087222
 [8,] 2.783155 2.232120 1.2940313 1.823810 2.891021 1.8346436 3.536088
 [9,] 2.300886 2.567662 1.2002483 2.224262 2.623019 1.8329427 3.223934
[10,] 2.345922 2.103801 1.4733256 1.801321 2.664459 1.7837057 2.810596
          [,8]     [,9]    [,10]
 [1,] 2.783155 2.300886 2.345922
 [2,] 2.232120 2.567662 2.103801
 [3,] 1.294031 1.200248 1.473326
 [4,] 1.823810 2.224262 1.801321
 [5,] 2.891021 2.623019 2.664459
 [6,] 1.834644 1.832943 1.783706
 [7,] 3.536088 3.223934 2.810596
 [8,] 3.133619 2.283068 1.976192
 [9,] 2.283068 2.591424 1.922565
[10,] 1.976192 1.922565 2.373093</code></pre>
<pre class="r"><code>f</code></pre>
<pre><code>           [,1]
 [1,] 17.011594
 [2,] 15.836859
 [3,]  8.421969
 [4,] 12.709194
 [5,] 16.483775
 [6,] 10.403704
 [7,] 19.057971
 [8,] 13.873418
 [9,] 12.736734
[10,] 11.963858</code></pre>
<pre class="r"><code>solgradF&lt;- grad(F, f, x0f, maxIts, tol,xf)
solgradF</code></pre>
<pre><code>              X        X0         resid iter
 [1,] 0.6172931 0.6173013 -8.226232e-06 3532
 [2,] 0.8323071 0.8323258 -1.870730e-05 3532
 [3,] 0.5397727 0.5397052  6.745871e-05 3532
 [4,] 0.1785781 0.1785965 -1.844572e-05 3532
 [5,] 0.7888260 0.7887850  4.097310e-05 3532
 [6,] 0.3950113 0.3949807  3.059918e-05 3532
 [7,] 0.7050482 0.7050369  1.135881e-05 3532
 [8,] 0.8651250 0.8651531 -2.812421e-05 3532
 [9,] 0.1558820 0.1558699  1.212071e-05 3532
[10,] 0.3583763 0.3584488 -7.249651e-05 3532</code></pre>
<p>At this point I wasn’t ready to actually do any analysis I was merely getting everything prepared for analysis. I decided to see how Gradient method compares to gradient method with least squares. From my understanding of Linear algebra and my explanation of least squares methods. Gradient method of least squares should be faster.</p>
</div>
</div>
<div id="gradient-descent-with-least-squares" class="section level1">
<h1>Gradient Descent with Least Squares</h1>
<pre class="r"><code># defines the function
gradLS &lt;- function (A,b,x0a, maxIts, tol,xa)
  {
    # calculates n
    n &lt;- length(b)
    # creates an initial residual
    r &lt;- b - (A %*% x0a)
    # used to keep track of the number of iterations.
    k = 0
    # loop used to attempt to find the approximation of the X vector
    while (k &lt; maxIts &amp;&amp; norm(r, &quot;1&quot;) &gt; tol)
    {
      # increments the iteration
      k &lt;- k+1
      # creates a step size using the residuals and matrix A
      alpha &lt;- (t(r) %*% r) / (t(A%*%r) %*% (A %*% r))
      # creates the next approximation of the X vector
      x0a &lt;- x0a + drop(alpha) * r
      # creates the new residual with the new approximation
      r &lt;- t(A)%*% b - (t(A) %*% A) %*% x0a
    }
  #Calculates the residual vector.
  res &lt;- xa - x0a
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol &lt;- cbind(xa, x0a, res, matrix(rep(k,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) &lt;- c(&quot;X&quot;, &quot;X0&quot;,&quot;resid&quot;, &quot;iter&quot;)
  # prints the array
  sol
 
}</code></pre>
<div id="initial-5x5-a" class="section level3">
<h3>Initial 5x5 (A)</h3>
<pre class="r"><code>solgradLSA&lt;- gradLS(A, a, x0a, maxIts, tol, xa)
solgradLSA</code></pre>
<pre><code>              X         X0        resid iter
[1,] 0.04876597 0.05866985 -0.009903879 9996
[2,] 0.14752424 0.14450011  0.003024130 9996
[3,] 0.81504717 0.80794311  0.007104065 9996
[4,] 0.16098630 0.16652532 -0.005539026 9996
[5,] 0.07040319 0.06549751  0.004905672 9996</code></pre>
<p>Huston there is a problem. This method is supposed to be faster, but 325 is much faster than 9996. Maybe its the algorithm. If we adjust the residual vector maybe that will fix it.</p>
<pre class="r"><code># defines the function
gradLS2 &lt;- function (A,b,x0a, maxIts, tol, xa)
  {
    # Determines N
    n &lt;- length(b)
    # creates an initial residual
    r &lt;- b - (A %*% x0a)
    # used to keep track of the number of iterations.
    k = 0
    # loop used to attempt to find the approximation of the X vector
    while (k &lt; maxIts &amp;&amp; norm(r, &quot;1&quot;) &gt; tol)
    {
      # increments the iteration
      k &lt;- k+1
      # creates a step size using the residuals and matrix A
      alpha &lt;- (t(r) %*% r) / (t(A%*%r) %*% (A %*% r))
      # creates the next approximation of the X vector
      x0a &lt;- x0a + drop(alpha) * r
      # creates the new residual with the new approximation
      r &lt;-  b - (A) %*% x0a
    }
  #Calculates the residual vector.
  res &lt;- xa - x0a
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol &lt;- cbind(xa, x0a, res, matrix(rep(k,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) &lt;- c(&quot;X&quot;, &quot;X0&quot;,&quot;resid&quot;, &quot;iter&quot;)
  # prints the array
  sol
}</code></pre>
</div>
<div id="same-matrix" class="section level2">
<h2>Same Matrix</h2>
<pre class="r"><code>solgradLS2A&lt;- gradLS2(A, a, x0a, maxIts, tol,xa)
solgradLS2A</code></pre>
<pre><code>              X         X0         resid iter
[1,] 0.04876597 0.04892393 -1.579616e-04  567
[2,] 0.14752424 0.14747914  4.510033e-05  567
[3,] 0.81504717 0.81493841  1.087606e-04  567
[4,] 0.16098630 0.16105485 -6.855431e-05  567
[5,] 0.07040319 0.07032542  7.776373e-05  567</code></pre>
<p>That’s better than the the Gradient Descent Least Squares but still not better than the original Gradient Descent function. Lets test it on a few more matrices and see if this is true for them all.</p>
<pre class="r"><code>solgradLSB&lt;- gradLS(B, b, x0c, maxIts, tol,xb)
solgradLSC&lt;- gradLS(C, c, x0c, maxIts, tol,xc)
solgradLSD&lt;- gradLS(D, d, x0d, maxIts, tol,xd)
solgradLSE&lt;- gradLS(E, e, x0e, maxIts, tol,xe)
solgradLSF&lt;- gradLS(F, f, x0f, maxIts, tol,xf)
solgradLS2B&lt;- gradLS2(B, b, x0c, maxIts, tol,xb)
solgradLS2C&lt;- gradLS2(C, c, x0c, maxIts, tol,xc)
solgradLS2D&lt;- gradLS2(D, d, x0d, maxIts, tol,xd)
solgradLS2E&lt;- gradLS2(E, e, x0e, maxIts, tol,xe)
solgradLS2F&lt;- gradLS2(F, f, x0f, maxIts, tol,xf)
iterations &lt;- c(solgradA[1,4], solgradB[1,4],solgradC[1,4],solgradD[1,4],solgradE[1,4],solgradF[1,4])
iterations &lt;- rbind(iterations,c(solgradLSA[1,4], solgradLSB[1,4],solgradLSC[1,4],solgradLSD[1,4],solgradLSE[1,4], solgradLSF[1,4]))
iterations &lt;- rbind(iterations,c(solgradLS2A[1,4], solgradLS2B[1,4],solgradLS2C[1,4],solgradLS2D[1,4], solgradLS2E[1,4], solgradLS2F[1,4]))
rownames(iterations) &lt;- c(&quot;Steep&quot;, &quot;SteepLS&quot;, &quot;(mod)SteepLS&quot;)
colnames(iterations) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;)
iterations</code></pre>
<pre><code>                A      B      C     D     E      F
Steep         325   1781   1419  1515   322   3532
SteepLS      9996 877346 198154 30014 10737 914619
(mod)SteepLS  567   1431    685  6978   247    886</code></pre>
<p>We can see that for matrix A, D, and E the Gradient Method is fastest. For B, C, and F the modded least squares method is fastest. Why though? This still doesn’t make sense from my knowledge of linear Algebra. So, lets look at another method of iteration. Maybe it will help make some more sense of this.</p>
</div>
</div>
<div id="advanced-cheby-chev-iterative-method" class="section level1">
<h1>Advanced Cheby Chev iterative method</h1>
<p>After some digging and searching I finally found some code that helped solve the solution of <span class="math inline">\(A\vec{x} = \vec{b}\)</span> using the Cheby Chev advanced iterative method. How is this different than the other methods? Well all of these methods are iterative methods but Cheby Chev iterative method is avoids the use of a ratio of inner products. In order to avoid this ratio it has to create an elipse using the max and minimum eigen values. This means more knowledge of our original matrix must be known and this may not be as effective with larger matrices. For simplicity when creating this function I design it in a way that it solves for the max <span class="math inline">\(\lambda\)</span> and the min <span class="math inline">\(\lambda\)</span> which are our optimal values for solving this with Cheby Chev method. If I were to work on something more large scale I would use these as inputs because I would want to seperate these computations.</p>
<div id="the-function" class="section level2">
<h2>The function</h2>
<pre class="r"><code>cheby &lt;- function (A,b,x0a, maxIts, tol,xa)
{
  # Determines n
  n&lt;- length(b)
  # Creates vectors of eigen values and vectors
  ev &lt;- eigen(A)
  # Determines lambda max
  lmax&lt;- max(ev$values)
  # Determines lambda min
  lmin &lt;- min(ev$values)
  
  # Longest eliptical axis
  d&lt;- (lmax +lmin)/2.0
  # Shortest eliptical axis
  c&lt;- (lmax-lmin)/2.0
  # Creates a preconditioning matrix
  precon &lt;- diag(length(A[,1]))
  #Stores initial approximation vector
  x &lt;- x0a
  #Creates an inital residual vector
  r &lt;- b - A %*% x
  # tracks iterations
  i &lt;- 1
  
  while(i &lt; maxIts &amp;&amp; norm(r, &quot;1&quot;) &gt; tol)
  {
    # solving the preconditioned matrix
    z = solve(precon, r)
    if (i == 1)
    {
      # Intial start
      p &lt;- z
      # Initial step size
      alpha &lt;- 1/d
    }
    else
    {
      beta &lt;- (c*alpha/2.0)^2
      alpha &lt;- 1/(d-beta/alpha)
      p &lt;- z + beta*p
    }
    x &lt;- x + alpha*p
    r &lt;- b - A %*% x
    i = i+1
  }
  #Calculates the residual vector.
  res &lt;- xa - x
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol &lt;- cbind(xa, x, res, matrix(rep(i-1,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) &lt;- c(&quot;X&quot;, &quot;X0&quot;,&quot;resid&quot;, &quot;iter&quot;)
  # prints the array
  sol
}</code></pre>
<p>Now that the function is created lets get it running and see how it compares to the previous methods.</p>
</div>
<div id="x5-matrix-a" class="section level2">
<h2>5x5 Matrix (A)</h2>
<pre class="r"><code>solchebyA&lt;- cheby(A, a, x0a, maxIts, tol, xa)
solchebyA</code></pre>
<pre><code>              X         X0         resid iter
[1,] 0.04876597 0.04876585  1.179932e-07  160
[2,] 0.14752424 0.14752382  4.187338e-07  160
[3,] 0.81504717 0.81504659  5.780416e-07  160
[4,] 0.16098630 0.16098635 -5.878102e-08  160
[5,] 0.07040319 0.07040275  4.342705e-07  160</code></pre>
</div>
<div id="x6-matrix-b" class="section level2">
<h2>6x6 Matrix (B)</h2>
<pre class="r"><code>solchebyB&lt;- cheby(B, b, x0b, maxIts, tol, xb)
solchebyB</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.2163319 0.2163321 -1.541916e-07  371
[2,] 0.6504235 0.6504236 -1.305528e-07  371
[3,] 0.3351660 0.3351661 -4.539997e-08  371
[4,] 0.5076559 0.5076560 -1.209017e-07  371
[5,] 0.6528394 0.6528395 -1.691811e-07  371
[6,] 0.9655767 0.9655768 -1.387843e-07  371</code></pre>
</div>
<div id="x6-matrix-c" class="section level2">
<h2>6x6 Matrix (C)</h2>
<pre class="r"><code>solchebyC&lt;- cheby(C, c, x0c, maxIts, tol, xc)
solchebyC</code></pre>
<pre><code>             X        X0        resid iter
[1,] 0.6807363 0.6807361 2.473449e-07  208
[2,] 0.1331188 0.1331185 2.458469e-07  208
[3,] 0.4157805 0.4157804 1.459222e-07  208
[4,] 0.8683657 0.8683655 2.053613e-07  208
[5,] 0.6214230 0.6214229 1.294426e-07  208
[6,] 0.1328657 0.1328657 8.116714e-08  208</code></pre>
</div>
<div id="x6-matrix-d" class="section level2">
<h2>6x6 Matrix (D)</h2>
<pre class="r"><code>solchebyD&lt;- cheby(D, d, x0d, maxIts, tol, xd)
solchebyD</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.6258299 0.6258300 -1.445057e-07  393
[2,] 0.2011419 0.2011421 -1.617106e-07  393
[3,] 0.5883483 0.5883484 -1.722459e-07  393
[4,] 0.8560448 0.8560449 -1.650346e-07  393
[5,] 0.9363852 0.9363854 -1.873318e-07  393
[6,] 0.9780356 0.9780357 -1.214472e-07  393</code></pre>
</div>
<div id="x6-matrix-e" class="section level2">
<h2>6x6 Matrix (E)</h2>
<pre class="r"><code>solchebyE&lt;- cheby(E, e, x0e, maxIts, tol, xe)
solchebyE</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.4544894 0.4544895 -2.017894e-08   95
[2,] 0.3672737 0.3672738 -1.448896e-07   95
[3,] 0.4045790 0.4045793 -2.819368e-07   95
[4,] 0.3278605 0.3278606 -5.023016e-08   95
[5,] 0.7741572 0.7741574 -2.078386e-07   95
[6,] 0.3875421 0.3875424 -2.725173e-07   95</code></pre>
</div>
<div id="x6-matrix-f" class="section level2">
<h2>6x6 Matrix (F)</h2>
<pre class="r"><code>solchebyF&lt;- cheby(F, f, x0f, maxIts, tol, xf)
solchebyF</code></pre>
<pre><code>              X        X0         resid iter
 [1,] 0.6172931 0.6172931 -4.760992e-08  325
 [2,] 0.8323071 0.8323071 -4.552779e-08  325
 [3,] 0.5397727 0.5397727 -1.791034e-08  325
 [4,] 0.1785781 0.1785781 -3.797272e-08  325
 [5,] 0.7888260 0.7888260 -4.208400e-08  325
 [6,] 0.3950113 0.3950113 -2.666612e-08  325
 [7,] 0.7050482 0.7050483 -5.197652e-08  325
 [8,] 0.8651250 0.8651250 -4.009958e-08  325
 [9,] 0.1558820 0.1558820 -3.507980e-08  325
[10,] 0.3583763 0.3583763 -3.911280e-08  325</code></pre>
<pre class="r"><code>iterations &lt;- rbind(iterations,c(solchebyA[1,4], solchebyB[1,4],solchebyC[1,4],solchebyD[1,4], solchebyE[1,4], solchebyF[1,4]))
rownames(iterations) &lt;- c(&quot;Steep&quot;, &quot;SteepLS&quot;, &quot;(mod)SteepLS&quot;, &quot;Cheby Chev&quot;)
colnames(iterations) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;)
iterations</code></pre>
<pre><code>                A      B      C     D     E      F
Steep         325   1781   1419  1515   322   3532
SteepLS      9996 877346 198154 30014 10737 914619
(mod)SteepLS  567   1431    685  6978   247    886
Cheby Chev    160    371    208   393    95    325</code></pre>
<p>Well this method is clearly faster then all methods previously discussed. This is interesting because it is not the dominantly used method. Why though? I am to assume that on larger matrices knowledge and calculation of eigenvalues might be expensive or taxing. At this point I decided I needed to discuss this with Dr. Katrina Palmer and see why. We talked about it for a minute and her first question was about the condition number of the matrices. Time to explore condition numbers.</p>
</div>
</div>
<div id="condition-numbers-of-our-matrices" class="section level1">
<h1>Condition Numbers of our matrices</h1>
<pre class="r"><code>condition &lt;- c(kappa(A), kappa(B), kappa(C),kappa(D), kappa(E), kappa(F))
condition &lt;- rbind(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;), round(condition,0))
condition</code></pre>
<pre><code>     [,1]  [,2]   [,3]  [,4]   [,5]  [,6] 
[1,] &quot;A&quot;   &quot;B&quot;    &quot;C&quot;   &quot;D&quot;    &quot;E&quot;   &quot;F&quot;  
[2,] &quot;275&quot; &quot;1706&quot; &quot;434&quot; &quot;2148&quot; &quot;105&quot; &quot;675&quot;</code></pre>
<p>Well these functions are very ill-conditioned. This means that small changes can affect the approximation greatly which explains why the modded method works better than the normal least squares method. Since this may very well be the issue lets recondition all the matrices and see if this changes our assumption on which method is better.</p>
<div id="conditioning-the-matrices" class="section level2">
<h2>Conditioning the matrices</h2>
<p>To condition a matrix we simple add some multiple of the identity to the original matrix.</p>
<pre class="r"><code>precon &lt;- diag(length(a))*5
A &lt;- A + precon
a &lt;- A %*% xa 
kappa(A)</code></pre>
<pre><code>[1] 2.283249</code></pre>
<p>In the code chunk above, I preconditioned A then recalculated the solution vector <span class="math inline">\(\vec{a}\)</span> and printed the condition number. Now to repeat it for all of the matrices.</p>
<pre class="r"><code>precon &lt;- diag(length(b))*5
B &lt;- B + precon
b &lt;- B %*% xb 
kappa(B)</code></pre>
<pre><code>[1] 3.9592</code></pre>
<pre class="r"><code>precon &lt;- diag(length(c))*5
C &lt;- C + precon
c &lt;- C %*% xc 
kappa(C)</code></pre>
<pre><code>[1] 2.821978</code></pre>
<pre class="r"><code>precon &lt;- diag(length(d))*5
D &lt;- D + precon
d &lt;- D %*% xd 
kappa(D)</code></pre>
<pre><code>[1] 3.683559</code></pre>
<pre class="r"><code>precon &lt;- diag(length(e))*5
E &lt;- E + precon
e &lt;- E %*% xe 
kappa(E)</code></pre>
<pre><code>[1] 3.194292</code></pre>
<pre class="r"><code>precon &lt;- diag(length(f))*5
F &lt;- F + precon
f &lt;- F %*% xf 
kappa(F)</code></pre>
<pre><code>[1] 6.47289</code></pre>
<pre class="r"><code>condition &lt;- c(kappa(A), kappa(B), kappa(C),kappa(D), kappa(E), kappa(F))
condition &lt;- rbind(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;), round(condition,0))
condition</code></pre>
<pre><code>     [,1] [,2] [,3] [,4] [,5] [,6]
[1,] &quot;A&quot;  &quot;B&quot;  &quot;C&quot;  &quot;D&quot;  &quot;E&quot;  &quot;F&quot; 
[2,] &quot;2&quot;  &quot;4&quot;  &quot;3&quot;  &quot;4&quot;  &quot;3&quot;  &quot;6&quot; </code></pre>
<p>Much better condition numbers. Time to re-run all of the functions</p>
<pre class="r"><code>solgradA &lt;- grad(A, a, x0a, maxIts, tol, xa)
solgradB &lt;- grad(B, b, x0b, maxIts, tol, xb)
solgradC &lt;- grad(C, c, x0c, maxIts, tol, xc)
solgradD &lt;- grad(D, d, x0d, maxIts, tol, xd)
solgradE &lt;- grad(E, e, x0e, maxIts, tol, xe)
solgradF &lt;- grad(F, f, x0f, maxIts, tol, xf)
solgradLSA&lt;- gradLS(A, a, x0a, maxIts, tol, xa)
solgradLSB&lt;- gradLS(B, b, x0c, maxIts, tol,xb)
solgradLSC&lt;- gradLS(C, c, x0c, maxIts, tol,xc)
solgradLSD&lt;- gradLS(D, d, x0d, maxIts, tol,xd)
solgradLSE&lt;- gradLS(E, e, x0e, maxIts, tol,xe)
solgradLSF&lt;- gradLS(F, f, x0f, maxIts, tol,xf)
solgradLS2B&lt;- gradLS2(B, b, x0c, maxIts, tol,xb)
solgradLS2C&lt;- gradLS2(C, c, x0c, maxIts, tol,xc)
solgradLS2D&lt;- gradLS2(D, d, x0d, maxIts, tol,xd)
solgradLS2E&lt;- gradLS2(E, e, x0e, maxIts, tol,xe)
solgradLS2F&lt;- gradLS2(F, f, x0f, maxIts, tol,xf)
solchebyA&lt;- cheby(A, a, x0a, maxIts, tol, xa)
solchebyB&lt;- cheby(B, b, x0b, maxIts, tol, xb)
solchebyC&lt;- cheby(C, c, x0c, maxIts, tol, xc)
solchebyD&lt;- cheby(D, d, x0d, maxIts, tol, xd)
solchebyE&lt;- cheby(E, e, x0e, maxIts, tol, xe)
solchebyF&lt;- cheby(F, f, x0f, maxIts, tol, xf)

iterations &lt;- c(solgradA[1,4], solgradB[1,4],solgradC[1,4],solgradD[1,4],solgradE[1,4],solgradF[1,4])
iterations &lt;- rbind(iterations,c(solgradLSA[1,4], solgradLSB[1,4],solgradLSC[1,4],solgradLSD[1,4],solgradLSE[1,4], solgradLSF[1,4]))
iterations &lt;- rbind(iterations,c(solgradLS2A[1,4], solgradLS2B[1,4],solgradLS2C[1,4],solgradLS2D[1,4], solgradLS2E[1,4], solgradLS2F[1,4]))
iterations &lt;- rbind(iterations,c(solchebyA[1,4], solchebyB[1,4],solchebyC[1,4],solchebyD[1,4], solchebyE[1,4], solchebyF[1,4]))
rownames(iterations) &lt;- c(&quot;Steep&quot;, &quot;SteepLS&quot;, &quot;(mod)SteepLS&quot;, &quot;Cheby Chev&quot;)
colnames(iterations) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;)
iterations</code></pre>
<pre><code>               A   B  C   D  E   F
Steep         13  11 11  15 11  23
SteepLS       22  20 22  46 26 200
(mod)SteepLS 567 115 92 109 94 161
Cheby Chev    11  16 13  15 12  23</code></pre>
<p>By conditioning the matrix prior to solving using the iterative method we can see that it makes a significant difference in solving for the solution but Cheby Chev’s method is still close to the fastest method for solving these symmetric matrices. There is also a functional form that can be used for non-symmetric matrices. I want to consider using this on image processing and seeing where this takes me.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
