<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="Kory.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Kory Illenye</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Teaching
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Class_2.html">STT 2810</a>
    </li>
    <li>
      <a href="Class_1.html">MAT 1110</a>
    </li>
    <li>
      <a href="Resources.html">Resources</a>
    </li>
    <li>
      <a href="Numerical_blog.html">Numerical Blog</a>
    </li>
  </ul>
</li>
<li>
  <a href="Contact.html">Contact</a>
</li>
<li>
  <a href="http://mathsci.appstate.edu">Department of Mathematical Sciences</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<div id="steepest-descent-and-steepest-descent-with-least-squares." class="section level1">
<h1>Steepest Descent and Steepest Descent with Least Squares.</h1>
<p>Initially we started by looking at two iterative methods for finding solutions to positive definite, symmetric matrices. Steepest Descent is often called <strong>gradient descent</strong>, which is a approach to finding minima of a function <span class="math inline">\(f\)</span>. Using an estimate <span class="math inline">\(x_i\)</span>, the negative gradient <span class="math inline">\(-\nabla f(x_i)\)</span> gives the direction of in which <span class="math inline">\(f\)</span> is decreasing at the greatest rate. The goal is to take steps in this direction until we react the minimum.</p>
<p>Notation:</p>
<p><span class="math inline">\(e\)</span> denotes an error term</p>
<p><span class="math inline">\(i\)</span> denotes the i-ith term</p>
<p><span class="math inline">\(r\)</span> denotes a residual</p>
<p><span class="math display">\[e_i = x_i - x\]</span> <span class="math display">\[r_i = b - Ax_i = -Ae_i\]</span></p>
<p>To determine the size, often referred to as <span class="math inline">\(\alpha\)</span>, a ratio of inner products from residuals is used. <span class="math inline">\(x_{i+1} = x_i - \alpha_i\nabla f(x_i)\)</span> this minimizes <span class="math inline">\(f(x_{i+1})\)</span>. Now, that it is understood what is needed lets define a few things. To be symmetric means <span class="math inline">\(A = A^T\)</span> where <span class="math inline">\(A^T\)</span> is the transpose of A. Positive definite means all of the matrixâ€™s eigen values are greater than 0.</p>
<div id="creating-a-positive-definite-symetric-matrix" class="section level2">
<h2>Creating a positive definite, symetric matrix</h2>
<p>Defining initial variables.</p>
<pre class="r"><code># Setting the seed to work with 
set.seed(31)
# Number of columns and rows in the matrix in the matrix
n &lt;- 5 
# Maximum number of iterations for functions
maxIts &lt;- 1000000
# Setting a tolerance for use with the function
tol = 10^-5
# Creates a random nxn matrix with n^2 randomly generated numbers
A&lt;-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
A &lt;- t(A) %*% A 
# Creates a random nx1 solution vector to solve for
xa &lt;- matrix(runif(n), ncol = 1)
# Creates an nx1 b vector from the random vector x
a &lt;- A %*% xa
# Initial approximation vector (zero vecor)
x0a &lt;- matrix(rep(0.0,n), ncol = 1)
# Determines if all the eigen values are positive
is.positive.definite(A)</code></pre>
<pre><code>[1] TRUE</code></pre>
<p>The return of TRUE from the code block above allows us to know that the matrix is positive definite. Lets see what these objects look like and then design the function.</p>
<div id="the-matrix-a" class="section level3">
<h3>The Matrix A</h3>
<pre class="r"><code>#prints the matrix A
A</code></pre>
<pre><code>          [,1]      [,2]      [,3]      [,4]      [,5]
[1,] 2.4299775 1.7269373 2.0199944 0.5590044 1.5099956
[2,] 1.7269373 1.8590925 1.1651029 0.4614977 1.1855963
[3,] 2.0199944 1.1651029 1.8962176 0.5224703 1.2304534
[4,] 0.5590044 0.4614977 0.5224703 0.3137412 0.4209155
[5,] 1.5099956 1.1855963 1.2304534 0.4209155 1.0294046</code></pre>
</div>
<div id="the-b-vector" class="section level3">
<h3>The b vector</h3>
<pre class="r"><code>#Prints b
a</code></pre>
<pre><code>          [,1]
[1,] 2.2159566
[2,] 1.4658554
[3,] 1.9866331
[4,] 0.6013222
[5,] 1.3916532</code></pre>
</div>
</div>
</div>
<div id="the-funtion-for-gradient-descent-method" class="section level1">
<h1>The Funtion for Gradient Descent Method</h1>
<pre class="r"><code># defines the function
grad &lt;- function (A,b,x0a, maxIts, tol, x)
  {
    # calculates n
    n &lt;- length(b)
    # creates an initial residual
    r &lt;- b - (A %*% x0a)
    # used to keep track of the number of iterations.
    k = 0
    # loop used to attempt to find the approximation of the X vector
    while (k &lt; maxIts &amp;&amp; norm(r, &quot;1&quot;) &gt; tol)
    {
      # increments the iteration
      k &lt;- k+1
      # creates a step size using the residuals and matrix A
      alpha &lt;- (t(r) %*% r) / (t(r) %*% (A %*% r))
      # creates the next approximation of the X vector
      x0a &lt;- x0a + drop(alpha) * r
      # creates the new residual with the new approximation
      r &lt;- b - A %*% x0a
    }
  #Calculates the residual vector.
  res &lt;- x - x0a
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol &lt;- cbind(x, x0a, res, matrix(rep(k,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) &lt;- c(&quot;X&quot;, &quot;X0&quot;,&quot;resid&quot;, &quot;iter&quot;)
  # prints the array
  sol
 
}</code></pre>
<p>This code when run generates the following:</p>
<pre class="r"><code># runs the function and saves it to a variable &quot;c&quot;
sol1 &lt;- grad(A, a, x0a, maxIts, tol, xa)
# prints the array 
sol1</code></pre>
<pre><code>              X         X0         resid iter
[1,] 0.04876597 0.04895561 -1.896346e-04  325
[2,] 0.14752424 0.14746647  5.776931e-05  325
[3,] 0.81504717 0.81491134  1.358297e-04  325
[4,] 0.16098630 0.16109232 -1.060204e-04  325
[5,] 0.07040319 0.07030940  9.378641e-05  325</code></pre>
<p>You will notice that the code is not random and reproduces the same number every time. This is because a random seed was selected for reproduction of results. The <span class="math inline">\(X\)</span> column represents the exact solution, where the <span class="math inline">\(X0\)</span> column represents the approximation from the function. The resid column shows the actual difference between the two vectors. As you can see the approximation is very close to the actual solution. The norm of the residual is <span class="math inline">\(10^5\)</span> when rounded to 5 decimal places. For this document we know that if the <code>iter</code> column is not equal to 10^{6} then the maximum norm of the residual vector is <span class="math inline">\(10^5\)</span>. The final column just represents the number of iterations it actually took the function before converging to the approximate solution. You will notice that each row has an out put but this does <strong>not</strong> mean each row took that many iterations. Simply was concatenated this way. On the above matrix it took the function 325 iterations to complete. It worked for a <span class="math inline">\(5x5\)</span> matrix, now to apply it to a few larger matrices.</p>
<div id="x6-positive-definite-symmetric-matrix" class="section level2">
<h2>6x6 Positive Definite Symmetric Matrix</h2>
<pre class="r"><code>set.seed(21)
n &lt;- 6 
# Creates a Random matrix with n^2 randomly generated numbers
B&lt;-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
B &lt;- t(B) %*% B
# Creates a random solution vector to solve for
xb &lt;- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
b &lt;- B %*% xb
# initial approximation vector (zero vecor)
x0b &lt;- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(B)</code></pre>
<pre><code>[1] TRUE</code></pre>
<div id="the-matrix-and-b-vector" class="section level3">
<h3>The Matrix and b vector</h3>
<pre class="r"><code>B</code></pre>
<pre><code>         [,1]      [,2]      [,3]     [,4]     [,5]     [,6]
[1,] 2.969523 2.4693875 1.0945040 2.265522 2.268699 2.549656
[2,] 2.469388 3.0539191 0.9314847 2.069346 2.489122 2.389834
[3,] 1.094504 0.9314847 0.7736687 1.114188 1.197342 1.040629
[4,] 2.265522 2.0693460 1.1141883 2.475676 2.296733 2.158044
[5,] 2.268699 2.4891220 1.1973418 2.296733 2.593211 2.497518
[6,] 2.549656 2.3898338 1.0406293 2.158044 2.497518 3.180717</code></pre>
<pre class="r"><code>b</code></pre>
<pre><code>         [,1]
[1,] 7.708481
[2,] 7.815830
[3,] 3.454047
[4,] 7.049440
[5,] 7.781529
[6,] 8.252008</code></pre>
</div>
<div id="the-results" class="section level3">
<h3>The Results</h3>
<pre class="r"><code>sol2&lt;-grad(B, b, x0b, maxIts, tol,xb)
sol2</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.2163319 0.2164881 -0.0001561833 1781
[2,] 0.6504235 0.6502254  0.0001980371 1781
[3,] 0.3351660 0.3348874  0.0002786669 1781
[4,] 0.5076559 0.5075127  0.0001432318 1781
[5,] 0.6528394 0.6532767 -0.0004373487 1781
[6,] 0.9655767 0.9654451  0.0001315399 1781</code></pre>
<p>For the purpose of this document random seeds have been set for replication and discussion but many of random matrices were sampled repeatedly but not reported. Each of the following seeds were created by a random 10-sided dice and adding a digit to the previous seed.</p>
</div>
</div>
<div id="another-6x6-positive-definite-symmetric-matrix" class="section level2">
<h2>Another 6x6 Positive Definite Symmetric Matrix</h2>
<pre class="r"><code>set.seed(214)
n &lt;- 6 
# Creates a Random matrix with n^2 randomly generated numbers
C&lt;-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
C &lt;- t(C) %*% C
# Creates a random solution vector to solve for
xc &lt;- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
c &lt;- C %*% xc
# initial approximation vector (zero vecor)
x0c &lt;- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(C)</code></pre>
<pre><code>[1] TRUE</code></pre>
<div id="the-matrix-and-b-vector-1" class="section level3">
<h3>The Matrix and b vector</h3>
<pre class="r"><code>C</code></pre>
<pre><code>         [,1]     [,2]      [,3]      [,4]      [,5]      [,6]
[1,] 2.591707 2.217206 1.1814162 1.5399616 1.6181376 1.1860408
[2,] 2.217206 2.650669 1.6894440 2.0397363 1.4540539 1.1738115
[3,] 1.181416 1.689444 1.3483715 1.3084386 0.9020424 0.5422738
[4,] 1.539962 2.039736 1.3084386 2.0160104 1.2108004 0.8519311
[5,] 1.618138 1.454054 0.9020424 1.2108004 1.4474137 0.4330670
[6,] 1.186041 1.173812 0.5422738 0.8519311 0.4330670 0.9206222</code></pre>
<pre class="r"><code>c</code></pre>
<pre><code>         [,1]
[1,] 5.051013
[2,] 5.395403
[3,] 3.358559
[4,] 4.480104
[5,] 3.678552
[6,] 2.320329</code></pre>
</div>
<div id="the-results-1" class="section level3">
<h3>The Results</h3>
<pre class="r"><code>sol3&lt;-grad(C, c, x0c, maxIts, tol,xc)
sol3</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.6807363 0.6806130  1.233517e-04 1419
[2,] 0.1331188 0.1331240 -5.250738e-06 1419
[3,] 0.4157805 0.4157955 -1.500242e-05 1419
[4,] 0.8683657 0.8683055  6.016602e-05 1419
[5,] 0.6214230 0.6215572 -1.341803e-04 1419
[6,] 0.1328657 0.1330050 -1.392501e-04 1419</code></pre>
</div>
</div>
<div id="another-6x6-positive-definite-symmetric-matrix-1" class="section level2">
<h2>Another 6x6 Positive Definite Symmetric Matrix</h2>
<pre class="r"><code>set.seed(2145)
n &lt;- 6 
# Creates a Random matrix with n^2 randomly generated numbers
D&lt;-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
D &lt;- t(D) %*% D
# Creates a random solution vector to solve for
xd &lt;- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
d &lt;- D %*% xd
# initial approximation vector (zero vecor)
x0d &lt;- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(D)</code></pre>
<pre><code>[1] TRUE</code></pre>
<div id="the-matrix-and-b-vector-2" class="section level3">
<h3>The Matrix and b vector</h3>
<pre class="r"><code>D</code></pre>
<pre><code>         [,1]      [,2]      [,3]     [,4]     [,5]     [,6]
[1,] 1.629180 1.3253076 1.3704596 1.897338 1.492470 1.388390
[2,] 1.325308 2.1268329 0.9046964 2.031244 1.432492 1.145834
[3,] 1.370460 0.9046964 1.6388220 1.852486 1.570883 1.469332
[4,] 1.897338 2.0312444 1.8524862 2.844479 2.251959 1.748982
[5,] 1.492470 1.4324924 1.5708827 2.251959 2.753722 1.775626
[6,] 1.388390 1.1458340 1.4693319 1.748982 1.775626 1.692876</code></pre>
<pre class="r"><code>d</code></pre>
<pre><code>         [,1]
[1,] 6.472100
[2,] 5.990356
[3,] 6.497666
[4,] 8.940155
[5,] 8.389341
[6,] 6.779419</code></pre>
</div>
<div id="the-results-2" class="section level3">
<h3>The Results</h3>
<pre class="r"><code>sol4&lt;- grad(D, d, x0d, maxIts, tol,xd)
sol4</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.6258299 0.6258653 -3.544603e-05 1515
[2,] 0.2011419 0.2014055 -2.636294e-04 1515
[3,] 0.5883483 0.5887837 -4.354756e-04 1515
[4,] 0.8560448 0.8556266  4.181889e-04 1515
[5,] 0.9363852 0.9364996 -1.144587e-04 1515
[6,] 0.9780356 0.9777614  2.741660e-04 1515</code></pre>
</div>
</div>
<div id="another-6x6-positive-definite-symmetric-matrix-2" class="section level2">
<h2>Another 6x6 Positive Definite Symmetric Matrix</h2>
<pre class="r"><code>set.seed(21458)
n &lt;- 6 
# Creates a Random matrix with n^2 randomly generated numbers
E&lt;-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
E &lt;- t(E) %*% E
# Creates a random solution vector to solve for
xe &lt;- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
e &lt;- E %*% xe
# initial approximation vector (zero vecor)
x0e &lt;- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(E)</code></pre>
<pre><code>[1] TRUE</code></pre>
<div id="the-matrix-and-b-vector-3" class="section level3">
<h3>The Matrix and b vector</h3>
<pre class="r"><code>E</code></pre>
<pre><code>          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]
[1,] 0.8516702 0.5918866 0.9644886 0.2638436 0.8583120 1.2921903
[2,] 0.5918866 1.5296755 1.6492970 0.1676149 1.6131301 1.1953723
[3,] 0.9644886 1.6492970 3.0691736 1.0978719 2.0166848 1.7108179
[4,] 0.2638436 0.1676149 1.0978719 0.9585854 0.3851128 0.4277445
[5,] 0.8583120 1.6131301 2.0166848 0.3851128 2.1906219 1.6998282
[6,] 1.2921903 1.1953723 1.7108179 0.4277445 1.6998282 2.3113791</code></pre>
<pre class="r"><code>e</code></pre>
<pre><code>         [,1]
[1,] 2.246422
[2,] 3.265114
[3,] 4.870010
[4,] 1.403840
[5,] 4.279366
[6,] 4.070408</code></pre>
</div>
<div id="the-results-3" class="section level3">
<h3>The Results</h3>
<pre class="r"><code>sol5&lt;- grad(E, e, x0e, maxIts, tol,xe)
sol5</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.4544894 0.4544451  4.431286e-05  322
[2,] 0.3672737 0.3672618  1.191118e-05  322
[3,] 0.4045790 0.4045915 -1.246805e-05  322
[4,] 0.3278605 0.3278489  1.160496e-05  322
[5,] 0.7741572 0.7741505  6.730072e-06  322
[6,] 0.3875421 0.3875714 -2.928487e-05  322</code></pre>
</div>
</div>
</div>
<div id="a-10x10-positive-definite-symmetric-matrix" class="section level1">
<h1>A 10x10 Positive Definite Symmetric Matrix</h1>
<pre class="r"><code>set.seed(21456)
n &lt;- 10 
# Creates a Random matrix with n^2 randomly generated numbers
F&lt;-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
F &lt;- t(F) %*% F
# Creates a random solution vector to solve for
xf &lt;- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
f &lt;- F %*% xf
# initial approximation vector (zero vecor)
x0f &lt;- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(F)</code></pre>
<pre><code>[1] TRUE</code></pre>
<div id="the-matrix-and-b-vector-4" class="section level3">
<h3>The Matrix and b vector</h3>
<pre class="r"><code>F</code></pre>
<pre><code>          [,1]     [,2]      [,3]     [,4]     [,5]      [,6]     [,7]
 [1,] 4.576307 3.544017 1.8706107 2.699453 3.464121 2.1412326 3.630429
 [2,] 3.544017 3.995759 1.7402252 2.908020 3.122860 1.7878213 3.701816
 [3,] 1.870611 1.740225 1.4091612 1.632820 1.585560 0.8950613 1.883202
 [4,] 2.699453 2.908020 1.6328204 3.177959 2.352454 1.6335524 2.982156
 [5,] 3.464121 3.122860 1.5855600 2.352454 3.832469 2.1929775 3.852252
 [6,] 2.141233 1.787821 0.8950613 1.633552 2.192978 2.0398368 2.512283
 [7,] 3.630429 3.701816 1.8832023 2.982156 3.852252 2.5122828 5.087222
 [8,] 2.783155 2.232120 1.2940313 1.823810 2.891021 1.8346436 3.536088
 [9,] 2.300886 2.567662 1.2002483 2.224262 2.623019 1.8329427 3.223934
[10,] 2.345922 2.103801 1.4733256 1.801321 2.664459 1.7837057 2.810596
          [,8]     [,9]    [,10]
 [1,] 2.783155 2.300886 2.345922
 [2,] 2.232120 2.567662 2.103801
 [3,] 1.294031 1.200248 1.473326
 [4,] 1.823810 2.224262 1.801321
 [5,] 2.891021 2.623019 2.664459
 [6,] 1.834644 1.832943 1.783706
 [7,] 3.536088 3.223934 2.810596
 [8,] 3.133619 2.283068 1.976192
 [9,] 2.283068 2.591424 1.922565
[10,] 1.976192 1.922565 2.373093</code></pre>
<pre class="r"><code>f</code></pre>
<pre><code>           [,1]
 [1,] 17.011594
 [2,] 15.836859
 [3,]  8.421969
 [4,] 12.709194
 [5,] 16.483775
 [6,] 10.403704
 [7,] 19.057971
 [8,] 13.873418
 [9,] 12.736734
[10,] 11.963858</code></pre>
<pre class="r"><code>sol6&lt;- grad(F, f, x0f, maxIts, tol,xf)
sol6</code></pre>
<pre><code>              X        X0         resid iter
 [1,] 0.6172931 0.6173013 -8.226232e-06 3532
 [2,] 0.8323071 0.8323258 -1.870730e-05 3532
 [3,] 0.5397727 0.5397052  6.745871e-05 3532
 [4,] 0.1785781 0.1785965 -1.844572e-05 3532
 [5,] 0.7888260 0.7887850  4.097310e-05 3532
 [6,] 0.3950113 0.3949807  3.059918e-05 3532
 [7,] 0.7050482 0.7050369  1.135881e-05 3532
 [8,] 0.8651250 0.8651531 -2.812421e-05 3532
 [9,] 0.1558820 0.1558699  1.212071e-05 3532
[10,] 0.3583763 0.3584488 -7.249651e-05 3532</code></pre>
</div>
</div>
<div id="gradient-descent-with-least-squares" class="section level1">
<h1>Gradient Descent with least squares</h1>
<p>Now that a few different matrices have been tested with gradient descent its time to look at gradient descent with least squares. This function is supposed to converge faster.</p>
<pre class="r"><code># defines the function
gradLS &lt;- function (A,b,x0a, maxIts, tol,xa)
  {
    # calculates n
    n &lt;- length(b)
    # creates an initial residual
    r &lt;- b - (A %*% x0a)
    # used to keep track of the number of iterations.
    k = 0
    # loop used to attempt to find the approximation of the X vector
    while (k &lt; maxIts &amp;&amp; norm(r, &quot;1&quot;) &gt; tol)
    {
      # increments the iteration
      k &lt;- k+1
      # creates a step size using the residuals and matrix A
      alpha &lt;- (t(r) %*% r) / (t(A%*%r) %*% (A %*% r))
      # creates the next approximation of the X vector
      x0a &lt;- x0a + drop(alpha) * r
      # creates the new residual with the new approximation
      r &lt;- t(A)%*% b - (t(A) %*% A) %*% x0a
    }
  #Calculates the residual vector.
  res &lt;- xa - x0a
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol &lt;- cbind(xa, x0a, res, matrix(rep(k,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) &lt;- c(&quot;X&quot;, &quot;X0&quot;,&quot;resid&quot;, &quot;iter&quot;)
  # prints the array
  sol
 
}</code></pre>
<div id="initial-5x5-a" class="section level3">
<h3>Initial 5x5 (A)</h3>
<pre class="r"><code>sol7&lt;- gradLS(A, a, x0a, maxIts, tol, xa)
sol7</code></pre>
<pre><code>              X         X0        resid iter
[1,] 0.04876597 0.05866985 -0.009903879 9996
[2,] 0.14752424 0.14450011  0.003024130 9996
[3,] 0.81504717 0.80794311  0.007104065 9996
[4,] 0.16098630 0.16652532 -0.005539026 9996
[5,] 0.07040319 0.06549751  0.004905672 9996</code></pre>
<p>Huston there is a problem. This method is supposed to be faster! 325 is much faster than 9996. Maybe its the algorithm. If we adjust the residual vector maybe that will fix it.</p>
<pre class="r"><code># defines the function
gradLS2 &lt;- function (A,b,x0a, maxIts, tol, xa)
  {
    # Determines N
    n &lt;- length(b)
    # creates an initial residual
    r &lt;- b - (A %*% x0a)
    # used to keep track of the number of iterations.
    k = 0
    # loop used to attempt to find the approximation of the X vector
    while (k &lt; maxIts &amp;&amp; norm(r, &quot;1&quot;) &gt; tol)
    {
      # increments the iteration
      k &lt;- k+1
      # creates a step size using the residuals and matrix A
      alpha &lt;- (t(r) %*% r) / (t(A%*%r) %*% (A %*% r))
      # creates the next approximation of the X vector
      x0a &lt;- x0a + drop(alpha) * r
      # creates the new residual with the new approximation
      r &lt;-  b - (A) %*% x0a
    }
  #Calculates the residual vector.
  res &lt;- xa - x0a
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol &lt;- cbind(xa, x0a, res, matrix(rep(k,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) &lt;- c(&quot;X&quot;, &quot;X0&quot;,&quot;resid&quot;, &quot;iter&quot;)
  # prints the array
  sol
}</code></pre>
</div>
<div id="same-matrix" class="section level2">
<h2>Same Matrix</h2>
<pre class="r"><code>sol7&lt;- gradLS2(A, a, x0a, maxIts, tol,xa)
sol7</code></pre>
<pre><code>              X         X0         resid iter
[1,] 0.04876597 0.04892393 -1.579616e-04  567
[2,] 0.14752424 0.14747914  4.510033e-05  567
[3,] 0.81504717 0.81493841  1.087606e-04  567
[4,] 0.16098630 0.16105485 -6.855431e-05  567
[5,] 0.07040319 0.07032542  7.776373e-05  567</code></pre>
<p>Thatâ€™s better than the the Gradient Descent Least Square but still not better than the original Gradient Descent function. Lets test it on a few more matrices and see if this is true for them all.</p>
<pre class="r"><code>sol8&lt;- gradLS(B, b, x0c, maxIts, tol,xb)
sol9&lt;- gradLS(C, c, x0c, maxIts, tol,xc)
sol10&lt;- gradLS(D, d, x0d, maxIts, tol,xd)
sol11&lt;- gradLS2(B, b, x0c, maxIts, tol,xb)
sol12&lt;- gradLS2(C, c, x0c, maxIts, tol,xc)
sol13&lt;- gradLS2(D, d, x0d, maxIts, tol,xd)
sol8</code></pre>
<pre><code>             X        X0       resid   iter
[1,] 0.2163319 0.2348287 -0.01849672 877346
[2,] 0.6504235 0.6269530  0.02347044 877346
[3,] 0.3351660 0.3021469  0.03301912 877346
[4,] 0.5076559 0.4906793  0.01697655 877346
[5,] 0.6528394 0.7046481 -0.05180877 877346
[6,] 0.9655767 0.9499844  0.01559222 877346</code></pre>
<pre class="r"><code>sol9</code></pre>
<pre><code>             X        X0         resid   iter
[1,] 0.6807363 0.6729057  0.0078306519 198154
[2,] 0.1331188 0.1334402 -0.0003213818 198154
[3,] 0.4157805 0.4167254 -0.0009448726 198154
[4,] 0.8683657 0.8645425  0.0038231974 198154
[5,] 0.6214230 0.6299234 -0.0085004329 198154
[6,] 0.1328657 0.1416898 -0.0088240857 198154</code></pre>
<pre class="r"><code>sol10</code></pre>
<pre><code>             X        X0        resid  iter
[1,] 0.6258299 0.6322439 -0.006413995 30014
[2,] 0.2011419 0.2488987 -0.047756839 30014
[3,] 0.5883483 0.6672407 -0.078892493 30014
[4,] 0.8560448 0.7802649  0.075779870 30014
[5,] 0.9363852 0.9571127 -0.020727566 30014
[6,] 0.9780356 0.9283533  0.049682312 30014</code></pre>
<pre class="r"><code>sol11</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.2163319 0.2164927 -0.0001607680 1431
[2,] 0.6504235 0.6502197  0.0002037872 1431
[3,] 0.3351660 0.3348793  0.0002867849 1431
[4,] 0.5076559 0.5075085  0.0001473855 1431
[5,] 0.6528394 0.6532895 -0.0004501359 1431
[6,] 0.9655767 0.9654413  0.0001353492 1431</code></pre>
<pre class="r"><code>sol12</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.6807363 0.6807517 -1.538168e-05  685
[2,] 0.1331188 0.1330651  5.366519e-05  685
[3,] 0.4157805 0.4158182 -3.764951e-05  685
[4,] 0.8683657 0.8683789 -1.323665e-05  685
[5,] 0.6214230 0.6214207  2.293070e-06  685
[6,] 0.1328657 0.1328826 -1.685239e-05  685</code></pre>
<pre class="r"><code>sol13</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.6258299 0.6258056  0.0000242890 6978
[2,] 0.2011419 0.2010149  0.0001269961 6978
[3,] 0.5883483 0.5881388  0.0002094314 6978
[4,] 0.8560448 0.8562503 -0.0002054745 6978
[5,] 0.9363852 0.9363277  0.0000575043 6978
[6,] 0.9780356 0.9781731 -0.0001375366 6978</code></pre>
<p>These matrices broke my theory again. Maybe the pseudo code is all wrong. Lets try another iterative method maybe the advanced Cheby Chev iterative method this method avoids the inner products of residuals.</p>
</div>
</div>
<div id="advanced-cheby-chev-iterative-method" class="section level1">
<h1>Advanced Cheby Chev iterative method</h1>
<p>After some research I found some pseudo code for this method.</p>
<div id="the-function" class="section level2">
<h2>The function</h2>
<pre class="r"><code>cheby &lt;- function (A,b,x0a, maxIts, tol,xa)
{
  # Determines n
  n&lt;- length(b)
  # Creates vectors of eigen values and vectors
  ev &lt;- eigen(A)
  # Determines lambda max
  lmax&lt;- max(ev$values)
  # Determines lambda min
  lmin &lt;- min(ev$values)
  
  # Longest eliptical axis
  d&lt;- (lmax +lmin)/2.0
  # Shortest eliptical axis
  c&lt;- (lmax-lmin)/2.0
  # Creates a preconditioning matrix
  precon &lt;- diag(length(A[,1]))
  #Stores initial approximation vector
  x &lt;- x0a
  #Creates an inital residual vector
  r &lt;- b - A %*% x
  # tracks iterations
  i &lt;- 1
  
  while(i &lt; maxIts &amp;&amp; norm(r, &quot;1&quot;) &gt; tol)
  {
    # solving the preconditioned matrix
    z = solve(precon, r)
    if (i == 1)
    {
      # Intial start
      p &lt;- z
      # Initial step size
      alpha &lt;- 1/d
    }
    else
    {
      beta &lt;- (c*alpha/2.0)^2
      alpha &lt;- 1/(d-beta/alpha)
      p &lt;- z + beta*p
    }
    x &lt;- x + alpha*p
    r &lt;- b - A %*% x
    i = i+1
  }
  #Calculates the residual vector.
  res &lt;- xa - x
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol &lt;- cbind(xa, x, res, matrix(rep(i-1,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) &lt;- c(&quot;X&quot;, &quot;X0&quot;,&quot;resid&quot;, &quot;iter&quot;)
  # prints the array
  sol
}</code></pre>
<p>lets see how it does?</p>
</div>
<div id="x5-matrix-a" class="section level2">
<h2>5x5 Matrix (A)</h2>
<pre class="r"><code>sol14&lt;- cheby(A, a, x0a, maxIts, tol, xa)
sol14</code></pre>
<pre><code>              X         X0         resid iter
[1,] 0.04876597 0.04876585  1.179932e-07  160
[2,] 0.14752424 0.14752382  4.187338e-07  160
[3,] 0.81504717 0.81504659  5.780416e-07  160
[4,] 0.16098630 0.16098635 -5.878102e-08  160
[5,] 0.07040319 0.07040275  4.342705e-07  160</code></pre>
<p>This was faster then all the other methods it appears. Time to investigate further.</p>
</div>
<div id="x6-matrix-b" class="section level2">
<h2>6x6 Matrix (B)</h2>
<pre class="r"><code>sol15&lt;- cheby(B, b, x0b, maxIts, tol, xb)
sol15</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.2163319 0.2163321 -1.541916e-07  371
[2,] 0.6504235 0.6504236 -1.305528e-07  371
[3,] 0.3351660 0.3351661 -4.539997e-08  371
[4,] 0.5076559 0.5076560 -1.209017e-07  371
[5,] 0.6528394 0.6528395 -1.691811e-07  371
[6,] 0.9655767 0.9655768 -1.387843e-07  371</code></pre>
</div>
<div id="x6-matrix-c" class="section level2">
<h2>6x6 Matrix (C)</h2>
<pre class="r"><code>sol16&lt;- cheby(C, c, x0c, maxIts, tol, xc)
sol16</code></pre>
<pre><code>             X        X0        resid iter
[1,] 0.6807363 0.6807361 2.473449e-07  208
[2,] 0.1331188 0.1331185 2.458469e-07  208
[3,] 0.4157805 0.4157804 1.459222e-07  208
[4,] 0.8683657 0.8683655 2.053613e-07  208
[5,] 0.6214230 0.6214229 1.294426e-07  208
[6,] 0.1328657 0.1328657 8.116714e-08  208</code></pre>
</div>
<div id="x6-matrix-d" class="section level2">
<h2>6x6 Matrix (D)</h2>
<pre class="r"><code>sol17&lt;- cheby(D, d, x0d, maxIts, tol, xd)
sol17</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.6258299 0.6258300 -1.445057e-07  393
[2,] 0.2011419 0.2011421 -1.617106e-07  393
[3,] 0.5883483 0.5883484 -1.722459e-07  393
[4,] 0.8560448 0.8560449 -1.650346e-07  393
[5,] 0.9363852 0.9363854 -1.873318e-07  393
[6,] 0.9780356 0.9780357 -1.214472e-07  393</code></pre>
</div>
<div id="x6-matrix-e" class="section level2">
<h2>6x6 Matrix (E)</h2>
<pre class="r"><code>sol18&lt;- cheby(E, e, x0e, maxIts, tol, xe)
sol18</code></pre>
<pre><code>             X        X0         resid iter
[1,] 0.4544894 0.4544895 -2.017894e-08   95
[2,] 0.3672737 0.3672738 -1.448896e-07   95
[3,] 0.4045790 0.4045793 -2.819368e-07   95
[4,] 0.3278605 0.3278606 -5.023016e-08   95
[5,] 0.7741572 0.7741574 -2.078386e-07   95
[6,] 0.3875421 0.3875424 -2.725173e-07   95</code></pre>
</div>
<div id="x6-matrix-f" class="section level2">
<h2>6x6 Matrix (F)</h2>
<pre class="r"><code>sol18&lt;- cheby(F, f, x0f, maxIts, tol, xf)
sol18</code></pre>
<pre><code>              X        X0         resid iter
 [1,] 0.6172931 0.6172931 -4.760992e-08  325
 [2,] 0.8323071 0.8323071 -4.552779e-08  325
 [3,] 0.5397727 0.5397727 -1.791034e-08  325
 [4,] 0.1785781 0.1785781 -3.797272e-08  325
 [5,] 0.7888260 0.7888260 -4.208400e-08  325
 [6,] 0.3950113 0.3950113 -2.666612e-08  325
 [7,] 0.7050482 0.7050483 -5.197652e-08  325
 [8,] 0.8651250 0.8651250 -4.009958e-08  325
 [9,] 0.1558820 0.1558820 -3.507980e-08  325
[10,] 0.3583763 0.3583763 -3.911280e-08  325</code></pre>
<p>After talking to Dr.Â Palmer I realized that part of the problem might be that the matrices are ill conditioned. So, lets redefine a few matrices and look at the condition numbers.</p>
<p>Well higher condition numbers mean that the matrices are ill conditioned. We can see that matrix B is highly ill conditioned. Well lets condition a matrix and see how that affects the number of iterations. We can use the B matrix.</p>
<p>By conditioning the matrix prior to solving using the iterative method we can see that it makes a significant difference in solving for the solution but Cheby Chevâ€™s method is still close to the fastest method for solving these symmetric matrices. There is also a functional form that can be used for non-symmetric matrices. I want to consider using this on image processing and seeing where this takes me.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
