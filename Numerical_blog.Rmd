---
title: ""
output:  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, prompt = FALSE, comment = NA)
library(matrixcalc)
```

# Steepest Descent and Steepest Descent with Least Squares.

Initially we started by looking at two iterative methods for finding solutions to positive definite, symmetric matrices. Steepest Descent is often called **gradient descent**, which is a approach to finding minima of a function $f$. Using an estimate $x_i$, the negative gradient $-\nabla f(x_i)$ gives the direction of in which $f$ is decreasing at the greatest rate. The goal is to take steps in this direction until we react the minimum. 

Notation:

$e$ denotes an error term

$i$ denotes the i-ith term

$r$ denotes a residual

$$e_i = x_i - x$$
$$r_i = b - Ax_i = -Ae_i$$

To determine the size, often referred to as $\alpha$, a ratio of inner products from residuals is used. $x_{i+1} = x_i - \alpha_i\nabla f(x_i)$ this minimizes $f(x_{i+1})$. Now, that it is understood what is needed lets define a few things. To be symmetric means $A = A^T$ where $A^T$ is the transpose of A. Positive definite means all of the matrix's eigen values are greater than 0.

## Creating a positive definite, symetric matrix

Defining initial variables.

```{r}
# Setting the seed to work with 
set.seed(31)
# Number of columns and rows in the matrix in the matrix
n <- 5 
# Maximum number of iterations for functions
maxIts <- 1000000
# Setting a tolerance for use with the function
tol = 10^-5
# Creates a random nxn matrix with n^2 randomly generated numbers
A<-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
A <- t(A) %*% A 
# Creates a random nx1 solution vector to solve for
xa <- matrix(runif(n), ncol = 1)
# Creates an nx1 b vector from the random vector x
a <- A %*% xa
# Initial approximation vector (zero vecor)
x0a <- matrix(rep(0.0,n), ncol = 1)
# Determines if all the eigen values are positive
is.positive.definite(A)
```

The return of TRUE from the code block above allows us to know that the matrix is positive definite. Lets see what these objects look like and then design the function.

### The Matrix A

```{r}
#prints the matrix A
A
```

### The b vector

```{r}
#Prints b
a
```

# The Funtion for Gradient Descent Method

```{r}
# defines the function
grad <- function (A,b,x0a, maxIts, tol, x)
  {
    # calculates n
    n <- length(b)
    # creates an initial residual
    r <- b - (A %*% x0a)
    # used to keep track of the number of iterations.
    k = 0
    # loop used to attempt to find the approximation of the X vector
    while (k < maxIts && norm(r, "1") > tol)
    {
      # increments the iteration
      k <- k+1
      # creates a step size using the residuals and matrix A
      alpha <- (t(r) %*% r) / (t(r) %*% (A %*% r))
      # creates the next approximation of the X vector
      x0a <- x0a + drop(alpha) * r
      # creates the new residual with the new approximation
      r <- b - A %*% x0a
    }
  #Calculates the residual vector.
  res <- x - x0a
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol <- cbind(x, x0a, res, matrix(rep(k,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) <- c("X", "X0","resid", "iter")
  # prints the array
  sol
 
}
```

This code when run generates the following:

```{r}
# runs the function and saves it to a variable "c"
sol1 <- grad(A, a, x0a, maxIts, tol, xa)
# prints the array 
sol1
```

You will notice that the code is not random and reproduces the same number every time. This is because a random seed was selected for reproduction of results. The $X$ column represents the exact solution, where the $X0$ column represents the approximation from the function. The resid column shows the actual difference between the two vectors. As you can see the approximation is very close to the actual solution. The norm of the residual is $10^5$ when rounded to 5 decimal places. For this document we know that if the `iter` column is not equal to `r maxIts` then the maximum norm of the residual vector is $10^5$. The final column just represents the number of iterations it actually took the function before converging to the approximate solution. You will notice that each row has an out put but this does **not** mean each row took that many iterations. Simply was concatenated this way. On the above matrix it took the function `r sol1[1,4]` iterations to complete. It worked for a $5x5$ matrix, now to apply it to a few larger matrices.

## 6x6 Positive Definite Symmetric Matrix

```{r}
set.seed(21)
n <- 6 
# Creates a Random matrix with n^2 randomly generated numbers
B<-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
B <- t(B) %*% B
# Creates a random solution vector to solve for
xb <- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
b <- B %*% xb
# initial approximation vector (zero vecor)
x0b <- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(B)
```

### The Matrix and b vector

```{r}
B
b
```

### The Results

```{r}
sol2<-grad(B, b, x0b, maxIts, tol,xb)
sol2
```

For the purpose of this document random seeds have been set for replication and discussion but many of random matrices were sampled repeatedly but not reported. Each of the following seeds were created by a random 10-sided dice and adding a digit to the previous seed.

## Another 6x6 Positive Definite Symmetric Matrix

```{r}
set.seed(214)
n <- 6 
# Creates a Random matrix with n^2 randomly generated numbers
C<-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
C <- t(C) %*% C
# Creates a random solution vector to solve for
xc <- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
c <- C %*% xc
# initial approximation vector (zero vecor)
x0c <- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(C)
```

### The Matrix and b vector

```{r}
C
c
```

### The Results

```{r}
sol3<-grad(C, c, x0c, maxIts, tol,xc)
sol3
```

## Another 6x6 Positive Definite Symmetric Matrix

```{r}
set.seed(2145)
n <- 6 
# Creates a Random matrix with n^2 randomly generated numbers
D<-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
D <- t(D) %*% D
# Creates a random solution vector to solve for
xd <- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
d <- D %*% xd
# initial approximation vector (zero vecor)
x0d <- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(D)
```

### The Matrix and b vector

```{r}
D
d
```

### The Results

```{r}

sol4<- grad(D, d, x0d, maxIts, tol,xd)
sol4
```

## Another 6x6 Positive Definite Symmetric Matrix

```{r}
set.seed(21458)
n <- 6 
# Creates a Random matrix with n^2 randomly generated numbers
E<-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
E <- t(E) %*% E
# Creates a random solution vector to solve for
xe <- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
e <- E %*% xe
# initial approximation vector (zero vecor)
x0e <- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(E)
```

### The Matrix and b vector

```{r}
E
e
```

### The Results

```{r}
sol5<- grad(E, e, x0e, maxIts, tol,xe)
sol5
```



# A 10x10 Positive Definite Symmetric Matrix

```{r}
set.seed(21456)
n <- 10 
# Creates a Random matrix with n^2 randomly generated numbers
F<-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
F <- t(F) %*% F
# Creates a random solution vector to solve for
xf <- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
f <- F %*% xf
# initial approximation vector (zero vecor)
x0f <- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(F)
```

### The Matrix and b vector

```{r}
F
f
```

```{r}
sol6<- grad(F, f, x0f, maxIts, tol,xf)
sol6
```

 

# Gradient Descent with least squares

Now that a few diferrent matrices have been tested with gradient descent its time to look at gradient descent with least squares. This function is supposed to converge faster.

```{r}
# defines the function
gradLS <- function (A,b,x0a, maxIts, tol,xa)
  {
    # calculates n
    n <- length(b)
    # creates an initial residual
    r <- b - (A %*% x0a)
    # used to keep track of the number of iterations.
    k = 0
    # loop used to attempt to find the approximation of the X vector
    while (k < maxIts && norm(r, "1") > tol)
    {
      # increments the iteration
      k <- k+1
      # creates a step size using the residuals and matrix A
      alpha <- (t(r) %*% r) / (t(A%*%r) %*% (A %*% r))
      # creates the next approximation of the X vector
      x0a <- x0a + drop(alpha) * r
      # creates the new residual with the new approximation
      r <- t(A)%*% b - (t(A) %*% A) %*% x0a
    }
  #Calculates the residual vector.
  res <- xa - x0a
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol <- cbind(xa, x0a, res, matrix(rep(k,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) <- c("X", "X0","resid", "iter")
  # prints the array
  sol
 
}
```

### Initial 5x5 (A)
```{r}
sol7<- gradLS(A, a, x0a, maxIts, tol, xa)
sol7
```

Huston there is a problem. This method is supposed to be faster! `r sol1[1,4]` is much faster than `r sol7[1,4]`. Maybe its the algorythm. If we adjust the residual vector maybe that will fix it.


```{r}
# defines the function
gradLS2 <- function (A,b,x0a, maxIts, tol, xa)
  {
    # Determines N
    n <- length(b)
    # creates an initial residual
    r <- b - (A %*% x0a)
    # used to keep track of the number of iterations.
    k = 0
    # loop used to attempt to find the approximation of the X vector
    while (k < maxIts && norm(r, "1") > tol)
    {
      # increments the iteration
      k <- k+1
      # creates a step size using the residuals and matrix A
      alpha <- (t(r) %*% r) / (t(A%*%r) %*% (A %*% r))
      # creates the next approximation of the X vector
      x0a <- x0a + drop(alpha) * r
      # creates the new residual with the new approximation
      r <-  b - (A) %*% x0a
    }
  #Calculates the residual vector.
  res <- xa - x0a
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol <- cbind(xa, x0a, res, matrix(rep(k,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) <- c("X", "X0","resid", "iter")
  # prints the array
  sol
}
```

## Same Matrix

```{r}
sol7<- gradLS2(A, a, x0a, maxIts, tol,xa)
sol7
```

Thats better than the the Gradient Descent Least Square but still not better than the original Gradient Descent function. Lets test it on a few more matrices and see if this is true for them all.


```{r}
sol8<- gradLS(B, b, x0c, maxIts, tol,xb)
sol9<- gradLS(C, c, x0c, maxIts, tol,xc)
sol10<- gradLS(D, d, x0d, maxIts, tol,xd)
sol11<- gradLS2(B, b, x0c, maxIts, tol,xb)
sol12<- gradLS2(C, c, x0c, maxIts, tol,xc)
sol13<- gradLS2(D, d, x0d, maxIts, tol,xd)
sol8
sol9
sol10
sol11
sol12
sol13

```

These matrices broke my theory again. Maybe the pseudo code is all wrong. Lets try another iterative method maybe the advanced Cheby Chev iterative method this method avoids the inner products of residuals.

#Advanced Cheby Chev iterative method

After some research I found some psuedo code for this method. 

## The function

```{r}
cheby <- function (A,b,x0a, maxIts, tol,xa)
{
  # Determines n
  n<- length(b)
  # Creates vectors of eigen values and vectors
  ev <- eigen(A)
  # Determines lambda max
  lmax<- max(ev$values)
  # Determines lambda min
  lmin <- min(ev$values)
  
  # Longest eliptical axis
  d<- (lmax +lmin)/2.0
  # Shortest eliptical axis
  c<- (lmax-lmin)/2.0
  # Creates a preconditioning matrix
  precon <- diag(length(A[,1]))
  #Stores initial approximation vector
  x <- x0a
  #Creates an inital residual vector
  r <- b - A %*% x
  # tracks iterations
  i <- 1
  
  while(i < maxIts && norm(r, "1") > tol)
  {
    # solving the preconditioned matrix
    z = solve(precon, r)
    if (i == 1)
    {
      # Intial start
      p <- z
      # Initial step size
      alpha <- 1/d
    }
    else
    {
      beta <- (c*alpha/2.0)^2
      alpha <- 1/(d-beta/alpha)
      p <- z + beta*p
    }
    x <- x + alpha*p
    r <- b - A %*% x
    i = i+1
  }
  #Calculates the residual vector.
  res <- xa - x
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol <- cbind(xa, x, res, matrix(rep(i-1,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) <- c("X", "X0","resid", "iter")
  # prints the array
  sol
}

```

lets see how it does?

## 5x5 Matrix (A)

```{r}
sol14<- cheby(A, a, x0a, maxIts, tol, xa)
sol14
```

This was faster then all the other methods it appears. Time to investigate further.

## 6x6 Matrix (B)

```{r}
sol15<- cheby(B, b, x0b, maxIts, tol, xb)
sol15
```

## 6x6 Matrix (C)


```{r}
sol16<- cheby(C, c, x0c, maxIts, tol, xc)
sol16
```

## 6x6 Matrix (D)


```{r}
sol17<- cheby(D, d, x0d, maxIts, tol, xd)
sol17
```

## 6x6 Matrix (E)


```{r}
sol18<- cheby(E, e, x0e, maxIts, tol, xe)
sol18
```

## 6x6 Matrix (F)


```{r}
sol18<- cheby(F, f, x0f, maxIts, tol, xf)
sol18
```




After talking to Dr. Palmer I realized that part of the problem might be that the matrices are ill conditioned. So, lets redefine a few matrices and look at the condition numbers.









Well higher condition numbers mean that the matrices are ill conditioned. We can see that matrix B is highly ill conditioned. Well lets condition a matrix and see how that affects the number of iterations. We can use the B matrix.


By conditioning the matrix prior to solving using the iterative method we can see that it makes a significant difference in solving for the solution but Cheby Chev's method is still close to the fastest method for solving these symmetric matrices. There is also a functional form that can be used for non-symmetric matrices.  I want to consider using this on image processing and seeing where this takes me.