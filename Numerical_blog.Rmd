---
title: ""
output:  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, prompt = FALSE, comment = NA)
library(matrixcalc)
```

# Steepest Descent and Steepest Descent with Least Squares.

Initially we started by looking at two iterative methods for finding solutions to positive definite, symmetric matrices. Steepest Descent is often called **gradient descent**, which is a approach to finding minima of a function $f$. Using an estimate $\vec{x}_i$, the negative gradient $-\nabla f(x_i)$ gives the direction of in which $f$ is decreasing at the greatest rate. The goal is to take steps in this direction until we react the minimum. 

Notation:

$e$ denotes an error term

$i$ denotes the i-ith term

$r$ denotes a residual

$$\vec{e}_i = \vec{x}_i - \vec{x}$$
$$\vec{r}_i = \vec{b} - A\vec{x}_i = -A\vec{e}_i$$

To determine the size, often referred to as $\alpha$, a ratio of inner products from residuals is used. $\vec{x}_{i+1} = \vec{x}_i - \alpha_i\nabla f(\vec{x}_i)$ this minimizes $f(\vec{x}_{i+1})$. So lets define a few things. To be symmetric means $A = A^T$ where $A^T$ is the transpose of A. Positive definite means all of the matrix's eigen values are greater than 0.

## Creating a positive definite, symetric matrix

Defining initial variables.

```{r}
# Setting the seed to work with 
set.seed(31)
# Number of columns and rows in the matrix in the matrix
n <- 5 
# Maximum number of iterations for functions
maxIts <- 1000000
# Setting a tolerance for use with the function
tol = 10^-5
# Creates a random nxn matrix with n^2 randomly generated numbers
A<-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
A <- t(A) %*% A 
# Creates a random nx1 solution vector to solve for
xa <- matrix(runif(n), ncol = 1)
# Creates an nx1 b vector from the random vector x
a <- A %*% xa
# Initial approximation vector (zero vecor)
x0a <- matrix(rep(0.0,n), ncol = 1)
# Determines if all the eigen values are positive
is.positive.definite(A)
```

The return of TRUE from the code block above allows us to know that the matrix is positive definite. Lets see what these objects look like and then design the function.

### The Matrix A

```{r}
# Prints the matrix A
A
```

### The b vector

```{r}
# Prints b vector for A
a
```

Without question the code is giving us the desired outcome of a $5x5$ matrix and a $5x1$ solution vector.

# The Funtion for Gradient Descent Method

This function uses a ratio of residual vectors as a step size.

$$\alpha = \frac{<\vec{r}^T,\vec{r}>}{\vec{r}^TA\vec{r}}$$
Where $\vec{r}$ is the previous iterations residual vector.

We approximate $\vec{x}$ by the following equation: 

$$\vec{x_{n+1}} = \vec{x_n} + \alpha\vec{r}$$
Here is the code for gradient descent in `R`.

```{r}
# Defines the function
grad <- function (A,b,x0a, maxIts, tol, x)
  {
    # Calculates n
    n <- length(b)
    # Creates an initial residual
    r <- b - (A %*% x0a)
    # Used to keep track of the number of iterations.
    k = 0
    # Loop used to attempt to find the approximation of the X vector
    while (k < maxIts && norm(r, "1") > tol)
    {
      # Increments the iteration
      k <- k+1
      # Creates a step size using the residuals and matrix A
      alpha <- (t(r) %*% r) / (t(r) %*% (A %*% r))
      # Creates the next approximation of the X vector
      x0a <- x0a + drop(alpha) * r
      # Creates the new residual with the new approximation
      r <- b - A %*% x0a
    }
  # Calculates the residual vector.
  res <- x - x0a
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol <- cbind(x, x0a, res, matrix(rep(k,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) <- c("X", "X0","resid", "iter")
  # Prints the array
  sol
 
}
```

This code when run generates the following:

```{r}
# Runs the function and saves it to a variable "c"
solgradA <- grad(A, a, x0a, maxIts, tol, xa)
# Prints the array 
solgradA
```

You will notice that the code is not random and reproduces the same number every time. This is because a random seed was selected for reproduction of results. The $X$ column represents the exact solution, where the $X0$ column represents the approximation from the function. The resid column shows the actual difference between the two vectors. As you can see the approximation is very close to the actual solution. The norm of the residual is $10^5$ when rounded to 5 decimal places. For this document we know that if the `iter` column is not equal to `r maxIts` then the maximum norm of the residual vector is $10^5$. The final column just represents the number of iterations it actually took the function before converging to the approximate solution. You will notice that each row has an out put but this does **not** mean each row took that many iterations. Simply was concatenated this way. On the above matrix it took the function `r solgradA[1,4]` iterations to complete. It worked for a $5x5$ matrix, now to apply it to a few larger matrices.

## 6x6 Positive Definite Symmetric Matrix

```{r}
set.seed(21)
n <- 6 
# Creates a Random matrix with n^2 randomly generated numbers
B<-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
B <- t(B) %*% B
# Creates a random solution vector to solve for
xb <- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
b <- B %*% xb
# Initial approximation vector (zero vecor)
x0b <- matrix(rep(0.0,n), ncol = 1)
# Determines if all the eigen values are positive
is.positive.definite(B)
```

### The B Matrix and b vector

```{r}
B
b
```

### The Results

```{r}
solgradB<-grad(B, b, x0b, maxIts, tol,xb)
solgradB
```

For the purpose of this document random seeds have been set for replication and discussion but many of random matrices were sampled repeatedly but not reported. Each of the following seeds were created by a random 10-sided dice and adding a digit to the previous seed.

## Another 6x6 Positive Definite Symmetric Matrix

```{r}
set.seed(214)
n <- 6 
# Creates a Random matrix with n^2 randomly generated numbers
C<-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
C <- t(C) %*% C
# Creates a random solution vector to solve for
xc <- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
c <- C %*% xc
# initial approximation vector (zero vecor)
x0c <- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(C)
```

### The A Matrix and b vector

```{r}
C
c
```

### The Results

```{r}
solgradC<-grad(C, c, x0c, maxIts, tol,xc)
solgradC
```

## Another 6x6 Positive Definite Symmetric Matrix

```{r}
set.seed(2145)
n <- 6 
# Creates a Random matrix with n^2 randomly generated numbers
D<-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
D <- t(D) %*% D
# Creates a random solution vector to solve for
xd <- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
d <- D %*% xd
# initial approximation vector (zero vecor)
x0d <- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(D)
```

### The D Matrix and b vector

```{r}
D
d
```

### The Results

```{r}

solgradD<- grad(D, d, x0d, maxIts, tol,xd)
solgradD
```

## Another 6x6 Positive Definite Symmetric Matrix

```{r}
set.seed(21458)
n <- 6 
# Creates a Random matrix with n^2 randomly generated numbers
E<-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
E <- t(E) %*% E
# Creates a random solution vector to solve for
xe <- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
e <- E %*% xe
# initial approximation vector (zero vecor)
x0e <- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(E)
```

### The E Matrix and b vector

```{r}
E
e
```

### The Results

```{r}
solgradE<- grad(E, e, x0e, maxIts, tol,xe)
solgradE
```



# A 10x10 Positive Definite Symmetric Matrix

```{r}
set.seed(21456)
n <- 10 
# Creates a Random matrix with n^2 randomly generated numbers
F<-matrix(runif(n^2), ncol=n)
# Forces the matrix to be symmetric
F <- t(F) %*% F
# Creates a random solution vector to solve for
xf <- matrix(runif(n), ncol = 1)
# Creates a b vector from our random vector x
f <- F %*% xf
# initial approximation vector (zero vecor)
x0f <- matrix(rep(0.0,n), ncol = 1)
# determines if all the eigen values are positive
is.positive.definite(F)
```

### The F Matrix and b vector

```{r}
F
f
```

```{r}
solgradF<- grad(F, f, x0f, maxIts, tol,xf)
solgradF
```

 At this point I wasn't ready to actually do any analysis. I was merely getting everything prepared for analysis. I decided to see how gradient method compares to gradient method with least squares. From my understanding of linear algebra and my explanation of least squares methods. Gradient method of least squares should be faster.

# Gradient Descent with Least Squares

```{r}
# defines the function
gradLS <- function (A,b,x0a, maxIts, tol,xa)
  {
    # calculates n
    n <- length(b)
    # creates an initial residual
    r <- b - (A %*% x0a)
    # used to keep track of the number of iterations.
    k = 0
    # loop used to attempt to find the approximation of the X vector
    while (k < maxIts && norm(r, "1") > tol)
    {
      # increments the iteration
      k <- k+1
      # creates a step size using the residuals and matrix A
      alpha <- (t(r) %*% r) / (t(A%*%r) %*% (A %*% r))
      # creates the next approximation of the X vector
      x0a <- x0a + drop(alpha) * r
      # creates the new residual with the new approximation
      r <- t(A)%*% b - (t(A) %*% A) %*% x0a
    }
  #Calculates the residual vector.
  res <- xa - x0a
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol <- cbind(xa, x0a, res, matrix(rep(k,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) <- c("X", "X0","resid", "iter")
  # prints the array
  sol
 
}
```

### Initial 5x5 (A)
```{r}
solgradLSA<- gradLS(A, a, x0a, maxIts, tol, xa)
solgradLSA
```

Huston there is a problem. This method is supposed to be faster, but `r solgradA[1,4]` is much faster than `r solgradLSA[1,4]`. Maybe its the algorithm. If we adjust the residual vector maybe that will fix it.


```{r}
# defines the function
gradLS2 <- function (A,b,x0a, maxIts, tol, xa)
  {
    # Determines N
    n <- length(b)
    # creates an initial residual
    r <- b - (A %*% x0a)
    # used to keep track of the number of iterations.
    k = 0
    # loop used to attempt to find the approximation of the X vector
    while (k < maxIts && norm(r, "1") > tol)
    {
      # increments the iteration
      k <- k+1
      # creates a step size using the residuals and matrix A
      alpha <- (t(r) %*% r) / (t(A%*%r) %*% (A %*% r))
      # creates the next approximation of the X vector
      x0a <- x0a + drop(alpha) * r
      # creates the new residual with the new approximation
      r <-  b - (A) %*% x0a
    }
  #Calculates the residual vector.
  res <- xa - x0a
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol <- cbind(xa, x0a, res, matrix(rep(k,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) <- c("X", "X0","resid", "iter")
  # prints the array
  sol
}
```

## Same Matrix

```{r}
solgradLS2A<- gradLS2(A, a, x0a, maxIts, tol,xa)
solgradLS2A
```

That's better than the the gradient descent least squares but still not better than the original gradient descent function. Lets test it on a few more matrices and see if this is true for them all.


```{r}
solgradLSB<- gradLS(B, b, x0c, maxIts, tol,xb)
solgradLSC<- gradLS(C, c, x0c, maxIts, tol,xc)
solgradLSD<- gradLS(D, d, x0d, maxIts, tol,xd)
solgradLSE<- gradLS(E, e, x0e, maxIts, tol,xe)
solgradLSF<- gradLS(F, f, x0f, maxIts, tol,xf)
solgradLS2B<- gradLS2(B, b, x0c, maxIts, tol,xb)
solgradLS2C<- gradLS2(C, c, x0c, maxIts, tol,xc)
solgradLS2D<- gradLS2(D, d, x0d, maxIts, tol,xd)
solgradLS2E<- gradLS2(E, e, x0e, maxIts, tol,xe)
solgradLS2F<- gradLS2(F, f, x0f, maxIts, tol,xf)
iterations <- c(solgradA[1,4], solgradB[1,4],solgradC[1,4],solgradD[1,4],solgradE[1,4],solgradF[1,4])
iterations <- rbind(iterations,c(solgradLSA[1,4], solgradLSB[1,4],solgradLSC[1,4],solgradLSD[1,4],solgradLSE[1,4], solgradLSF[1,4]))
iterations <- rbind(iterations,c(solgradLS2A[1,4], solgradLS2B[1,4],solgradLS2C[1,4],solgradLS2D[1,4], solgradLS2E[1,4], solgradLS2F[1,4]))
rownames(iterations) <- c("Steep", "SteepLS", "(mod)SteepLS")
colnames(iterations) <- c("A", "B", "C", "D", "E", "F")
iterations

```

We can see that for matrix A, D, and E the gradient method is fastest. For B, C, and F the modded least squares method is fastest. Why though? This still doesn't make sense from my knowledge of linear Algebra. So, lets look at another method of iteration. Maybe it will help make some more sense of this. 

# Advanced Cheby Chev iterative method

After some digging and searching I finally found some code that helped solve the solution of $A\vec{x} = \vec{b}$ using the Cheby Chev advanced iterative method. How is this different than the other methods? Well all of these methods are iterative methods but Cheby Chev iterative method avoids the use of a ratio of inner products. In order to avoid this ratio it has to create an elipse using the max and minimum eigen values. This means more knowledge of our original matrix must be known and this may not be as effective with larger matrices. For simplicity when creating this function I design edit in a way that it solves for the max $\lambda$ and the min $\lambda$ which are our optimal values for solving this with Cheby Chev method. If I were to work on something more large scale I would use these as inputs because I would want to seperate these computations.

## The function

```{r}
cheby <- function (A,b,x0a, maxIts, tol,xa)
{
  # Determines n
  n<- length(b)
  # Creates vectors of eigen values and vectors
  ev <- eigen(A)
  # Determines lambda max
  lmax<- max(ev$values)
  # Determines lambda min
  lmin <- min(ev$values)
  
  # Longest eliptical axis
  d<- (lmax +lmin)/2.0
  # Shortest eliptical axis
  c<- (lmax-lmin)/2.0
  # Creates a preconditioning matrix
  precon <- diag(length(A[,1]))
  #Stores initial approximation vector
  x <- x0a
  #Creates an inital residual vector
  r <- b - A %*% x
  # tracks iterations
  i <- 1
  
  while(i < maxIts && norm(r, "1") > tol)
  {
    # solving the preconditioned matrix
    z = solve(precon, r)
    if (i == 1)
    {
      # Intial start
      p <- z
      # Initial step size
      alpha <- 1/d
    }
    else
    {
      beta <- (c*alpha/2.0)^2
      alpha <- 1/(d-beta/alpha)
      p <- z + beta*p
    }
    x <- x + alpha*p
    r <- b - A %*% x
    i = i+1
  }
  #Calculates the residual vector.
  res <- xa - x
  # vairable to store the exact X, the aproximated X and the number of iterations
  sol <- cbind(xa, x, res, matrix(rep(i-1,n),ncol = 1))
  # Names the colums of the new array
  colnames(sol) <- c("X", "X0","resid", "iter")
  # prints the array
  sol
}

```

Now that the function is created lets get it running and see how it compares to the previous methods.

## 5x5 Matrix (A)

```{r}
solchebyA<- cheby(A, a, x0a, maxIts, tol, xa)
solchebyA
```



## 6x6 Matrix (B)

```{r}
solchebyB<- cheby(B, b, x0b, maxIts, tol, xb)
solchebyB
```

## 6x6 Matrix (C)


```{r}
solchebyC<- cheby(C, c, x0c, maxIts, tol, xc)
solchebyC
```

## 6x6 Matrix (D)


```{r}
solchebyD<- cheby(D, d, x0d, maxIts, tol, xd)
solchebyD
```

## 6x6 Matrix (E)


```{r}
solchebyE<- cheby(E, e, x0e, maxIts, tol, xe)
solchebyE
```

## 6x6 Matrix (F)


```{r}
solchebyF<- cheby(F, f, x0f, maxIts, tol, xf)
solchebyF
```

```{r}
iterations <- rbind(iterations,c(solchebyA[1,4], solchebyB[1,4],solchebyC[1,4],solchebyD[1,4], solchebyE[1,4], solchebyF[1,4]))
rownames(iterations) <- c("Steep", "SteepLS", "(mod)SteepLS", "Cheby Chev")
colnames(iterations) <- c("A", "B", "C", "D", "E", "F")
iterations

```


Well this method is clearly faster then all methods previously discussed. This is interesting because it is not the dominantly used method. Why though? I am to assume that on larger matrices knowledge and calculation of eigenvalues might be expensive or taxing. At this point I decided I needed to discuss this with Dr. Katrina Palmer and see why. We talked about it for a minute and her first question was about the condition number of the matrices. Time to explore condition numbers.

# Condition Numbers of our matrices

```{r}
condition <- c(kappa(A), kappa(B), kappa(C),kappa(D), kappa(E), kappa(F))
condition <- rbind(c("A", "B", "C", "D", "E", "F"), round(condition,0))
condition
```

Well these functions are very ill-conditioned. This means that small changes can affect the approximation greatly which explains why the modded method works better than the normal least squares method. Since this may very well be the issue lets recondition all the matrices and see if this changes our assumption on which method is better.

## Conditioning the matrices

To condition a matrix we simple add some multiple of the identity to the original matrix.

```{r}
precon <- diag(length(a))*5
A <- A + precon
a <- A %*% xa 
kappa(A)
```

In the code chunk above, I preconditioned A then recalculated the solution vector $\vec{a}$ and printed the condition number. Now to repeat it for all of the matrices.

```{r}
precon <- diag(length(b))*5
B <- B + precon
b <- B %*% xb 
kappa(B)
```

```{r}
precon <- diag(length(c))*5
C <- C + precon
c <- C %*% xc 
kappa(C)
```

```{r}
precon <- diag(length(d))*5
D <- D + precon
d <- D %*% xd 
kappa(D)
```

```{r}
precon <- diag(length(e))*5
E <- E + precon
e <- E %*% xe 
kappa(E)
```

```{r}
precon <- diag(length(f))*5
F <- F + precon
f <- F %*% xf 
kappa(F)
```

```{r}
condition <- c(kappa(A), kappa(B), kappa(C),kappa(D), kappa(E), kappa(F))
condition <- rbind(c("A", "B", "C", "D", "E", "F"), round(condition,0))
condition
```

Much better condition numbers. Time to re-run all of the functions

```{r}
solgradA <- grad(A, a, x0a, maxIts, tol, xa)
solgradB <- grad(B, b, x0b, maxIts, tol, xb)
solgradC <- grad(C, c, x0c, maxIts, tol, xc)
solgradD <- grad(D, d, x0d, maxIts, tol, xd)
solgradE <- grad(E, e, x0e, maxIts, tol, xe)
solgradF <- grad(F, f, x0f, maxIts, tol, xf)
solgradLSA<- gradLS(A, a, x0a, maxIts, tol, xa)
solgradLSB<- gradLS(B, b, x0c, maxIts, tol,xb)
solgradLSC<- gradLS(C, c, x0c, maxIts, tol,xc)
solgradLSD<- gradLS(D, d, x0d, maxIts, tol,xd)
solgradLSE<- gradLS(E, e, x0e, maxIts, tol,xe)
solgradLSF<- gradLS(F, f, x0f, maxIts, tol,xf)
solgradLS2B<- gradLS2(B, b, x0c, maxIts, tol,xb)
solgradLS2C<- gradLS2(C, c, x0c, maxIts, tol,xc)
solgradLS2D<- gradLS2(D, d, x0d, maxIts, tol,xd)
solgradLS2E<- gradLS2(E, e, x0e, maxIts, tol,xe)
solgradLS2F<- gradLS2(F, f, x0f, maxIts, tol,xf)
solchebyA<- cheby(A, a, x0a, maxIts, tol, xa)
solchebyB<- cheby(B, b, x0b, maxIts, tol, xb)
solchebyC<- cheby(C, c, x0c, maxIts, tol, xc)
solchebyD<- cheby(D, d, x0d, maxIts, tol, xd)
solchebyE<- cheby(E, e, x0e, maxIts, tol, xe)
solchebyF<- cheby(F, f, x0f, maxIts, tol, xf)

iterations <- c(solgradA[1,4], solgradB[1,4],solgradC[1,4],solgradD[1,4],solgradE[1,4],solgradF[1,4])
iterations <- rbind(iterations,c(solgradLSA[1,4], solgradLSB[1,4],solgradLSC[1,4],solgradLSD[1,4],solgradLSE[1,4], solgradLSF[1,4]))
iterations <- rbind(iterations,c(solgradLS2A[1,4], solgradLS2B[1,4],solgradLS2C[1,4],solgradLS2D[1,4], solgradLS2E[1,4], solgradLS2F[1,4]))
iterations <- rbind(iterations,c(solchebyA[1,4], solchebyB[1,4],solchebyC[1,4],solchebyD[1,4], solchebyE[1,4], solchebyF[1,4]))
rownames(iterations) <- c("Steep", "SteepLS", "(mod)SteepLS", "Cheby Chev")
colnames(iterations) <- c("A", "B", "C", "D", "E", "F")
iterations
```



By conditioning the matrix prior to solving using the iterative method we can see that it makes a significant difference in solving for the solution but Cheby Chev's method is still close to the fastest method for solving these symmetric matrices. There is also a functional form that can be used for non-symmetric matrices.  I want to consider using this on image processing and seeing where this takes me.


